{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MHPI *DeltaModel* + *hydroDL2.0* Tutorial: **$\\delta$ HBV**\n",
    "Last Revision: 7 Nov. 2024\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "In this tutorial, we’ll walk through a basic implementation of the generic differentiable modeling framework *DeltaModel*\n",
    "using the hydrology model [HBV](https://en.wikipedia.org/wiki/HBV_hydrology_model) from *hydroDL2.0* as a physics backbone.\n",
    "The resulting model we call $\\delta$ HBV (citation).\n",
    "    \n",
    "In general, a differentiable model is composed of a coupled physics model and neural network.\n",
    "Physics model includes parameters for which True values are unknown, but approximated in practice.\n",
    "By coupling a neural network, we can learn a set of parameters that can then be fed to the physics model\n",
    "alongside other known input variables to make predictions.\n",
    "\n",
    "In our case, we use physics model HBV and an LSTM parameterization network as the core of the differentiable model.\n",
    "Since the HBV model uses input forcing variables precipitation, temperature, and potential evapotranspiration (PET)\n",
    "with dimensions of days x sites, the LSTM will learn parameters at every site for every day. The structure of the\n",
    "differentiable model, therefore, looks like so:\n",
    "\n",
    "    Differentiable Model $\\delta$ HBV: \n",
    "    (inputs weather forcings, $x$, and basin attributes $A$)\n",
    "\n",
    "        learned parameters = LSTM($x$, $A$)\n",
    "        hydrologic predictions = HBV($x$, learned_parameters)\n",
    "    \n",
    "Currently, $\\delta$ HBV is setup to train and make hydrologic predictions on **streamflow**, but it can\n",
    "also be reconfigured without much effort to predict percolation, recharge, and groundwater flow, among others.\n",
    "\n",
    "\n",
    "After showing an example implementation, we'll demonstrate how to train the model and expose critical details of the process.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Setup\n",
    "**Code**: Before beginning, please see `examples/differentiable hydrology/setup_guide.md`\n",
    "for steps on how to setup DeltaModel, hydroDL2, and the included Conda ENV (`envs/deltamod_env.yaml`)\n",
    "\n",
    "**ENV** For a quick start, we recommend using the `deltamod` ENV, which includes a minimal list of\n",
    "all core DeltaModel dependencies.\n",
    "\n",
    "**Data** Preconfigured CAMELS data files can be shared on request, but will eventually be made available through our\n",
    "data engine `hydro_data_dev` along with expanded instructions on how to access.\n",
    "\n",
    "\n",
    "## 1. Creating a Model\n",
    "For this example, we demonstrate how to get DeltaModel running with minimal setup using the HBV implementation\n",
    "from hydroDL2. Note, there are two versions of $\\delta$ HBV which can be run here:\n",
    "\n",
    "- **$\\delta$ HBV 1.0**: Uses the standard version of the HBV model (citation).\n",
    "- **$\\delta$ HBV 1.1p**: An enhanced version of the HBV model, incorporating modifications like a capillary rise module to\n",
    "improve prediction performance over 1.0 (citation).\n",
    "\n",
    "Switching between these two versions requires only a simple configuration file change, as outlined below.\n",
    "\n",
    "\n",
    "\n",
    "- #### **Set Model and Experiment Configurations**\n",
    "    \n",
    "    Define your desired model settings and experimental parameters in a YAML configuration file. For this tutorial, two\n",
    "    configuration files have been premade for running both versions of $\\delta$ HBV. Each is configured to\n",
    "    use 531 CAMELS basins with 34 years of data available as inputs. For temporal tests, we have further configured\n",
    "    both files to run a 9-year training phase from 1999-2008 and 10-year test phase from 1989-1999 to reproduce benchmark\n",
    "    results of (citation):\n",
    "    -  **$\\delta$ HBV 1.0**: `example/conf/dhbv_config.yaml`\n",
    "    - **$\\delta$ HBV 1.1p**: `example/conf/dhbv_v1_1p_config.yaml` \n",
    "\n",
    "- #### **Building the Model**\n",
    "\n",
    "    Currently, differentiable models can be built with DeltaModel in two ways of varying flexibility:\n",
    "    - **Implicit**: This method is best for small-scale experiments and sharing final products.\n",
    "    \n",
    "        Add/change modules in DeltaModel to create your own differentiable model, and add to/change\n",
    "        configuration settings in `deltaModel/conf/config.yaml` to reflect the needs of your product. (Modules including\n",
    "        trainers, physics models, neural networks, loss functions, are designed to be hot-swappable per user needs. The\n",
    "        differentiable model modality will eventually also be made more flexible to meet diverse modeling needs. See\n",
    "        `docs/getting_started_guide` for more info on making the DeltaModel your own.) \n",
    "\n",
    "        With these pieces in place, simply run the model in your terminal using\n",
    "        ```shell\n",
    "        cd ./deltaModel\n",
    "        python __main__.py\n",
    "        ```\n",
    "\n",
    "    - **Explicit** (Code block below): This method is best for exploratory research and prototyping.\n",
    "\n",
    "        This approach is similar in that we still use a configuration file to hold model and experiment settings (though\n",
    "        a dictionary object can just as well be used). However, here we can expose the fundamental steps in the\n",
    "        modeling process; data preprocesing, model building, and experimentation/forwarding. In doing so, we make it\n",
    "        quicker to develop model and data pipelines, and easier to follow the processes within.\n",
    "\n",
    "\n",
    "    The example below represents a simple, explicit implementation of a differentiable model using the HBV backbone.\n",
    "    In this case we have the following steps\n",
    "    1.  **Load a configuration file**: Using Hydra and OmegaConf packages, we can convert our `..config.yaml` into a dictionary\n",
    "        object `config`. For example, if your config file contains `mode: train`, the dictionary yields\n",
    "        `config['mode'] == 'train'`. However, config can also contain sub-dictionaries. Take\n",
    "        ```yaml\n",
    "        training: \n",
    "            start_time: 1999/10/01\n",
    "        ```\n",
    "        for instance. In these cases, access the subdict like so `config['training']['start_time'] == '1999/10/01'`.\n",
    "    \n",
    "    2.  **Load in data**: At this step, we load in and process our data as a dictionary of attributes and feature datasets\n",
    "        that are used by the HBV model. In general, this dataset dict is supplied by the user, and should meet\n",
    "        minimum requirements established in `docs/getting_started_guide.md`.\n",
    "\n",
    "        In this example, we take a small, arbitrarily selected sample of the data to illustrate the modeling process.\n",
    "\n",
    "    3.  **Initialize sub-models**: Next, we initialize the physics model and neural network our differentiable model\n",
    "        will use, HBV and LSTM in this case.\n",
    "\n",
    "    4.  **Create a differentiable model**: Now, the sub-models are linked together by the DeltaModel wrapper. This\n",
    "        wrapper interfaces models using the desired modality, i.e., forwarding the LSTM to generate parameters that\n",
    "        it then feeds with forcing variables to HBV to generate predictions. \n",
    "\n",
    "    5.  **Forward/Experiment**: With the differentiable model created, it can be forwarded (as demonstrated\n",
    "        below), or trained/tested/applied in any user-defined experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streamflow predictions for 365 days and 100 basins: Showing the first 5 days for 5 basins \n",
      " tensor([[[1.3719e-05],\n",
      "         [1.6588e-05],\n",
      "         [1.3494e-05],\n",
      "         [1.8438e-05],\n",
      "         [1.6097e-05]],\n",
      "\n",
      "        [[3.3860e-05],\n",
      "         [3.8847e-05],\n",
      "         [3.3249e-05],\n",
      "         [4.1722e-05],\n",
      "         [3.7960e-05]],\n",
      "\n",
      "        [[5.3675e-05],\n",
      "         [5.9653e-05],\n",
      "         [5.2746e-05],\n",
      "         [6.2785e-05],\n",
      "         [5.8545e-05]],\n",
      "\n",
      "        [[7.0703e-05],\n",
      "         [7.6776e-05],\n",
      "         [6.9597e-05],\n",
      "         [7.9663e-05],\n",
      "         [7.5593e-05]],\n",
      "\n",
      "        [[8.4114e-05],\n",
      "         [8.9671e-05],\n",
      "         [8.2974e-05],\n",
      "         [9.2033e-05],\n",
      "         [8.8523e-05]]], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../deltaModel')  # Add the root directory of deltaModel\n",
    "\n",
    "from example import load_config \n",
    "from hydroDL2.models.hbv.hbv import HBVMulTDET as hbv\n",
    "from deltaModel.models.neural_networks import init_nn_model\n",
    "from deltaModel.core.data.dataset_loading import get_dataset_dict # Eventually a hydroData import\n",
    "from deltaModel.models.differentiable_model import DeltaModel as dHBV\n",
    "from deltaModel.core.data import take_sample\n",
    "\n",
    "\n",
    "\n",
    "CONFIG_PATH = '../conf/dhbv_config.yaml'\n",
    "\n",
    "\n",
    "\n",
    "# 1. Load configuration dictionary of model parameters and options\n",
    "config = load_config(CONFIG_PATH)\n",
    "\n",
    "# 2. Setup a dataset dictionary of NN and physics model inputs.\n",
    "# Take a sample to reduce size on GPU.\n",
    "dataset = get_dataset_dict(config, train=True)\n",
    "dataset_sample = take_sample(config, dataset, days=730, basins=100)\n",
    "\n",
    "# 3. Initialize physical model and neural network\n",
    "phy_model = hbv(config['dpl_model'])\n",
    "nn = init_nn_model(phy_model, config['dpl_model'])\n",
    "\n",
    "# 4. Create the differentiable model dHBV: \n",
    "# a torch.nn.Module that describes how nn is linked to the physical model.\n",
    "dpl_model = dHBV(phy_model, nn)\n",
    "\n",
    "## From here, dpl_model can be run or trained as any torch.nn.Module model in a\n",
    "## standard training loop.\n",
    "\n",
    "# 5. For example, to forward:\n",
    "output = dpl_model.forward(dataset_sample)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Streamflow predictions for {output['flow_sim'].shape[0]} days and {output['flow_sim'].shape[1]} basins: Showing the first 5 days for 5 basins \\n {output['flow_sim'][:5,:5]}\")  # TODO: Add a visualization here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 2. Training a Model\n",
    "\n",
    "Now that we can build a differentiable model, we will train $\\delta$ HBV (1.0 or 1.1p) and expose critical steps in the\n",
    "process, particularly within the model trainer.\n",
    "\n",
    "### 2.1 Load Config and Dataset\n",
    "\n",
    "Once again, we begin by loading in the config dict and CAMELS dataset.\n",
    "\n",
    "Be sure to change `CONFIG_PATH` to the HBV version you want to use.\n",
    "\n",
    "<!-- This code instantiates a model Trainer which will train or test a model per the user's specification in the config. Note\n",
    "that `__main__.py` is trimmed-down here to illustrate it's primary objective. The trainer performs the following\n",
    "functions:\n",
    "- CAMELS data will be loaded and preprocessed,\n",
    "- A differenial model object with the HBV1.1p backbone will be created, and \n",
    "- An optimizer and loss function will be initialized. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../deltaModel')  # Add the root directory of deltaModel\n",
    "\n",
    "from example import load_config \n",
    "from deltaModel.core.data.dataset_loading import get_dataset_dict # Eventually a hydroData import\n",
    "\n",
    "\n",
    "\n",
    "CONFIG_PATH = '../conf/dhbv_config.yaml'\n",
    "\n",
    "\n",
    "\n",
    "# Load configuration dictionary of model parameters and options\n",
    "config = load_config(CONFIG_PATH)\n",
    "\n",
    "# Setup a dataset dictionary of NN and physics model inputs.\n",
    "# Take a sample to reduce size on GPU.\n",
    "dataset = get_dataset_dict(config, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Initialize a $\\delta$ HBV Differentiable Model, Optimizer, and Loss Function\n",
    "\n",
    "These are the auxillary tasks completed by the Trainer before beginning the training loop.\n",
    "\n",
    "\n",
    "<!-- We use the Adadelta optimizer from PyTorch, feeding it both learnable model\n",
    "parameters and a learning rate from the config file.\n",
    "\n",
    "\n",
    "Dynamically load the loss function identified in the config (RMSE for\n",
    "dHBV 1.0 and NSE for dHBV 1.1p). -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is our dHBV framework: \n",
      " ----- \n",
      " DeltaModel(\n",
      "  (phy_model): HBVMulTDET()\n",
      "  (nn_model): CudnnLstmModel(\n",
      "    (linearIn): Linear(in_features=38, out_features=256, bias=True)\n",
      "    (lstm): CudnnLstm()\n",
      "    (linearOut): Linear(in_features=256, out_features=210, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from hydroDL2.models.hbv.hbv import HBVMulTDET as hbv\n",
    "from deltaModel.models.loss_functions import get_loss_fn\n",
    "from deltaModel.models.neural_networks import init_nn_model\n",
    "from deltaModel.models.differentiable_model import DeltaModel as dHBV\n",
    "\n",
    "\n",
    "\n",
    "# Initialize physical model and neural network\n",
    "phy_model = hbv(config['dpl_model'])\n",
    "nn = init_nn_model(phy_model, config['dpl_model'])\n",
    "\n",
    "# Create the differentiable model dHBV: \n",
    "# a torch.nn.Module that describes how nn is linked to the physical model.\n",
    "dpl_model = dHBV(phy_model, nn)\n",
    "print(f\"Here is our dHBV framework: \\n ----- \\n {dpl_model}\")\n",
    "\n",
    "# Init an Adadelta optimizer\n",
    "optimizer = torch.optim.Adadelta(\n",
    "    dpl_model.parameters(),\n",
    "    lr=config['dpl_model']['nn_model']['learning_rate']\n",
    "    )\n",
    "\n",
    "# init a loss function\n",
    "loss_fn = get_loss_fn(config, dataset['obs'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Train the Model\n",
    "\n",
    "Below we use a basic training loop to train the LSTM in $\\delta$ HBV to optimize HBV's parameters and improve its\n",
    "streamflow predictions.\n",
    "\n",
    "\n",
    "#### Key Steps in the Training Loop\n",
    "1. **Calculate Training Parameters**  \n",
    "   The `calc_training_params` function calculates the training settings:\n",
    "   - `n_sites`: The number of unique locations/sites in the dataset.\n",
    "   - `n_minibatch`: The number of samples to process per epoch.\n",
    "   - `n_timesteps`: The number of timesteps per sample.\n",
    "\n",
    "2. **Epoch Loop**  \n",
    "   Each epoch represents one full cycle through the training data. For each epoch:\n",
    "   - `total_loss` is reset to track the total error across all batches within the epoch.\n",
    "\n",
    "3. **Batch Loop**  \n",
    "   Within each epoch, the code processes data in smaller chunks (minibatches) to improve training efficiency and avoid\n",
    "   oversaturation of GPU VRAM. \n",
    "   \n",
    "   For each batch:\n",
    "   - **Sample Data**: `take_sample_train` randomly selects a sample of training data for the batch.\n",
    "   - **Forward Pass**: The model processes the input data to produce predictions.\n",
    "   - **Calculate Loss**: `loss_fn` compares predictions to observed values to calculate the error for the batch.\n",
    "   - **Backward Pass and Optimization**: \n",
    "     - `loss.backward()` computes gradients to adjust the model’s parameters.\n",
    "     - `optimizer.step()` updates the LSTM parameters.\n",
    "     - `optimizer.zero_grad()` resets gradients for the next batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'phy_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 9\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_model\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Batch size, number of training samples per epoch, and number of timesteps\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m n_sites, n_minibatch, n_timesteps \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_training_params\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minputs_nn_scaled\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_t_range\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Start of training.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[0;32m/data/lgl5139/project_blue_eyes/generic_diffModel/example/differentiable_hydrology/../../deltaModel/core/data/__init__.py:33\u001b[0m, in \u001b[0;36mcalc_training_params\u001b[0;34m(x, t_range, config, ngrid)\u001b[0m\n\u001b[1;32m     27\u001b[0m t \u001b[38;5;241m=\u001b[39m trange_to_array(t_range)\n\u001b[1;32m     28\u001b[0m rho \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(t\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdpl_model\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrho\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     29\u001b[0m n_iter_ep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\n\u001b[1;32m     30\u001b[0m     np\u001b[38;5;241m.\u001b[39mceil(\n\u001b[1;32m     31\u001b[0m         np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m rho \u001b[38;5;241m/\u001b[39m ngrid\n\u001b[0;32m---> 33\u001b[0m                  \u001b[38;5;241m/\u001b[39m (nt \u001b[38;5;241m-\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mphy_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarm_up\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     34\u001b[0m     )\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ngrid, n_iter_ep, nt,\n",
      "\u001b[0;31mKeyError\u001b[0m: 'phy_model'"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "from core.data import calc_training_params, take_sample_train\n",
    "from core.utils import save_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Batch size, number of training samples per epoch, and number of timesteps\n",
    "n_sites, n_minibatch, n_timesteps = calc_training_params(\n",
    "    dataset['inputs_nn_scaled'],\n",
    "    config['train_t_range'],\n",
    "    config\n",
    "    )\n",
    "\n",
    "# Start of training.\n",
    "for epoch in range(1, config['train']['epochs'] + 1):\n",
    "    total_loss = 0.0 # Initialize epoch loss to zero.\n",
    "\n",
    "    # Work through training data in batches.\n",
    "    prog_str = f\"Epoch {epoch}/{config['train']['epochs']}\"\n",
    "    \n",
    "    for i in tqdm.tqdm(range(1, n_minibatch + 1), desc=prog_str,\n",
    "                       leave=False, dynamic_ncols=True):\n",
    "        \n",
    "        # Take a sample of the training data for the batch.\n",
    "        dataset_sample = take_sample_train(config, dataset, n_sites, n_timesteps)\n",
    "\n",
    "        # Forward pass through dPL model.\n",
    "        predictions = dpl_model.forward(config, dataset_sample)        \n",
    "        \n",
    "        # Calculate loss.\n",
    "        loss = loss_fn(config,\n",
    "                       predictions,\n",
    "                       dataset_sample['obs'],\n",
    "                       igrid=dataset_sample['iGrid']\n",
    "                       )\n",
    "                                   \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / n_minibatch + 1\n",
    "    print(f\"Avg model loss after epoch {epoch}: {avg_loss}\")\n",
    "\n",
    "    # Save the model every save_epoch (set in the config).\n",
    "    model_name = config['dpl_model']['phy_model']['model_name']\n",
    "    if epoch % config['train']['save_epoch'] == 0:\n",
    "        save_model(config, dpl_model, model_name, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 3 Test a Model\n",
    "\n",
    "If you have run testing on a trained model and want to view the results, you can find a `mstd.csv` file in your\n",
    "`results/` directory, which will present performance statistics for the respective model. \n",
    "\n",
    "*A testing tutorial and Graphical visualizations of model output will be supported in a future updated.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydrodl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
