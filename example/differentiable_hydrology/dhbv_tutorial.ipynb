{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MHPI *DeltaModel* + *hydroDL2.0* Tutorial: **$\\delta$ HBV**\n",
    "Last Revision: 7 Nov. 2024\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "In this tutorial, we’ll walk through a basic implementation of the generic differentiable modeling framework *DeltaModel*\n",
    "using the hydrology model [HBV](https://en.wikipedia.org/wiki/HBV_hydrology_model) from *hydroDL2.0* as a physics backbone.\n",
    "The resulting model we call $\\delta$ HBV (citation).\n",
    "    \n",
    "In general, the differentiable model modality presented here is composed of a coupled physics model and neural network.\n",
    "Physics models include parameters for which True values are often unknown, but approximated to varying degrees in practice.\n",
    "By coupling a neural network, we can learn a set of a physics model's parameters, which can then be fed back in\n",
    "alongside other known input variables to make predictions.\n",
    "\n",
    "In this tutorial, we use the physics model HBV and an LSTM parameterization network as the core of a differentiable model.\n",
    "Since HBV uses input forcing variables precipitation, temperature, and potential evapotranspiration (PET)\n",
    "with dimensions of days $x$ sites, the LSTM will learn parameters at every site for every day. The structure of the\n",
    "differentiable model, therefore, looks like so:\n",
    "\n",
    "$\n",
    "\\delta \\text{HBV} = \n",
    "\\begin{cases}\n",
    "\\text{Learned Parameters = } \\text{LSTM}(x \\text{, } A) \\\\\n",
    "\\text{Hydrologic Predictions = } \\text{HBV}(x \\text{, Learned Parameters})\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "where\n",
    "- \\( $x$ \\) represents input weather forcings\n",
    "- \\( $A$ \\) is the set of basin attributes.\n",
    "\n",
    "    \n",
    "Currently, $\\delta$ HBV is setup to train and make hydrologic predictions on **streamflow**, but it can\n",
    "also be reconfigured without much effort to predict percolation, recharge, and groundwater flow, among others.\n",
    "\n",
    "\n",
    "After showing an example implementation, we'll demonstrate how to train the model and expose critical details of the process.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "## Setup\n",
    "- **Code**: Before beginning, please see `examples/differentiable hydrology/setup_guide.md`\n",
    "for steps on how to setup DeltaModel, hydroDL2, and the included Conda ENV (`envs/deltamod_env.yaml`)\n",
    "\n",
    "- **ENV**: For a quick start, we recommend using the `deltamod` ENV, which includes a minimal list of\n",
    "all core DeltaModel dependencies.\n",
    "\n",
    "- **Data**: Preconfigured CAMELS data files can be shared on request, but will eventually be made available through the MHPI\n",
    "data engine `hydro_data_dev` along with expanded instructions on how to access.\n",
    "\n",
    "<br>\n",
    "\n",
    "## 1. Creating a Model\n",
    "For this example, we demonstrate how to get DeltaModel running with minimal setup using the HBV implementation\n",
    "from hydroDL2. Note, there are two versions of $\\delta$ HBV which can be run here:\n",
    "\n",
    "- **$\\delta$ HBV 1.0**: Uses the standard version of the HBV model (citation).\n",
    "- **$\\delta$ HBV 1.1p**: An enhanced version of the HBV model, incorporating modifications like a capillary rise module to\n",
    "improve prediction performance over 1.0 (citation).\n",
    "\n",
    "Switching between these two versions requires only a simple configuration file change, as outlined below.\n",
    "\n",
    "<br>\n",
    "\n",
    "- #### **Set Model and Experiment Configurations**\n",
    "    \n",
    "    Define your desired model settings and experimental parameters in a YAML configuration file. For this tutorial, two\n",
    "    configuration files have been premade for running both versions of $\\delta$ HBV. Each is configured to\n",
    "    use 531 CAMELS basins with 34 years of data available as inputs. For temporal tests, we have further configured\n",
    "    both files to run a 9-year training phase from 1999-2008 and 10-year test phase from 1989-1999 to reproduce benchmark\n",
    "    results of (citation):\n",
    "    -  **$\\delta$ HBV 1.0**: `example/conf/dhbv_config.yaml`\n",
    "    - **$\\delta$ HBV 1.1p**: `example/conf/dhbv_v1_1p_config.yaml` \n",
    "\n",
    "- #### **Building the Model**\n",
    "\n",
    "    Currently, differentiable models can be built with DeltaModel in two ways of varying flexibility:\n",
    "    - **Implicit**: This method is best for small-scale experiments and distribution of final products.\n",
    "    \n",
    "        Add/change modules in DeltaModel to create your own differentiable model, and add to/change\n",
    "        configuration settings in `deltaModel/conf/config.yaml` to reflect the needs of your product. (Modules including\n",
    "        trainers, physics models, neural networks, loss functions, are designed to be hot-swappable per user needs. The\n",
    "        differentiable model modality will eventually also be made more flexible to meet diverse modeling needs. See\n",
    "        `docs/getting_started_guide` for more info on making the DeltaModel your own.) \n",
    "\n",
    "        With these pieces in place, simply run the model in your terminal using\n",
    "        ```shell\n",
    "        cd ./deltaModel\n",
    "        python __main__.py\n",
    "        ```\n",
    "\n",
    "    - **Explicit** (Code block below): This method is best for exploratory research and prototyping.\n",
    "\n",
    "        This approach is similar in that we still use a configuration file to hold model and experiment settings (though\n",
    "        a dictionary object can just as well be used). However, here we can expose the fundamental steps in the\n",
    "        modeling process; data preprocesing, model building, and experimentation/forwarding. In doing so, we make it\n",
    "        quicker to develop model and data pipelines, and easier to follow internal processes.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Walkthrough\n",
    "The example below represents a simple, explicit implementation of a differentiable model using the HBV backbone:\n",
    "1.  **Load a configuration file**: Using Hydra and OmegaConf packages, we can convert our `..config.yaml` into a dictionary\n",
    "    object `config`. For example, if your config file contains `mode: train`, the dictionary yields\n",
    "    `config['mode'] == 'train'`. However, config can also contain sub-dictionaries. Take\n",
    "    ```yaml\n",
    "    training: \n",
    "        start_time: 1999/10/01\n",
    "    ```\n",
    "    for instance. In these cases, accessing the subdict can be done like `config['training']['start_time'] == '1999/10/01'`.\n",
    "\n",
    "2.  **Load in data**: At this step, we load and process our data as a dictionary of variable and attribute datasets\n",
    "    that are used by the neural network and physics model. In general, this dataset dict is supplied by the user, and should meet\n",
    "    minimum requirements established in `docs/getting_started_guide.md`.\n",
    "\n",
    "    For this example, we take a small, arbitrarily selected sample of the data to illustrate the modeling process.\n",
    "\n",
    "3.  **Initialize sub-models**: Next, we initialize the physics model and neural network our differentiable model\n",
    "    will use, HBV and LSTM in this case.\n",
    "\n",
    "4.  **Create a differentiable model**: Now, the sub-models are linked together by the DeltaModel wrapper. This\n",
    "    wrapper interfaces models using the desired modality, i.e., forwarding the LSTM to generate parameters, which are then fed with\n",
    "    forcing variables back into HBV to generate predictions. \n",
    "\n",
    "6.  **Forward/Experiment**: With the differentiable model created, it can be forwarded (as demonstrated\n",
    "    below), or trained/tested/applied in any user-defined experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/lgl5139/project_blue_eyes/generic_diffModel/example/differentiable_hydrology/../../deltaModel/models/neural_networks/lstm_models.py:105: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /opt/conda/conda-bld/pytorch_1729647348947/work/aten/src/ATen/native/cudnn/RNN.cpp:1410.)\n",
      "  output, hy, cy, reserve, new_weight_buf = torch._cudnn_rnn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streamflow predictions for 365 days and 100 basins: Showing the first 5 days for 5 basins \n",
      " tensor([[[0.0184],\n",
      "         [0.0177],\n",
      "         [0.0118],\n",
      "         [0.0448],\n",
      "         [0.0335]],\n",
      "\n",
      "        [[0.0422],\n",
      "         [0.0391],\n",
      "         [0.0257],\n",
      "         [0.0990],\n",
      "         [0.0752]],\n",
      "\n",
      "        [[0.0640],\n",
      "         [0.0579],\n",
      "         [0.0380],\n",
      "         [0.1466],\n",
      "         [0.1124]],\n",
      "\n",
      "        [[0.0817],\n",
      "         [0.0726],\n",
      "         [0.0475],\n",
      "         [0.1839],\n",
      "         [0.1418]],\n",
      "\n",
      "        [[0.0947],\n",
      "         [0.0831],\n",
      "         [0.0544],\n",
      "         [0.2103],\n",
      "         [0.1630]]], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../')  # Add the root directory of deltaModel\n",
    "\n",
    "from example import load_config \n",
    "from hydroDL2.models.hbv.hbv import HBVMulTDET as hbv\n",
    "from deltaModel.models.neural_networks import init_nn_model\n",
    "from deltaModel.core.data.dataset_loading import get_dataset_dict # Eventually a hydroData import\n",
    "from deltaModel.models.differentiable_model import DeltaModel as dHBV\n",
    "from deltaModel.core.data import take_sample\n",
    "\n",
    "\n",
    "CONFIG_PATH = '../conf/dhbv_config.yaml'\n",
    "\n",
    "\n",
    "# 1. Load configuration dictionary of model parameters and options\n",
    "config = load_config(CONFIG_PATH)\n",
    "\n",
    "# 2. Setup a dataset dictionary of NN and physics model inputs.\n",
    "# Take a sample to reduce size on GPU.\n",
    "dataset = get_dataset_dict(config, train=True)\n",
    "dataset_sample = take_sample(config, dataset, days=730, basins=100)\n",
    "\n",
    "# 3. Initialize physical model and neural network\n",
    "phy_model = hbv(config['dpl_model'])\n",
    "nn = init_nn_model(phy_model, config['dpl_model'])\n",
    "\n",
    "# 4. Create the differentiable model dHBV: \n",
    "# a torch.nn.Module that describes how nn is linked to the physical model.\n",
    "dpl_model = dHBV(phy_model, nn)\n",
    "\n",
    "## From here, dpl_model can be run or trained as any torch.nn.Module model in a\n",
    "## standard training loop.\n",
    "\n",
    "# 5. For example, to forward:\n",
    "output = dpl_model.forward(dataset_sample)\n",
    "\n",
    "\n",
    "print(f\"Streamflow predictions for {output['flow_sim'].shape[0]} days and {output['flow_sim'].shape[1]} basins: Showing the first 5 days for 5 basins \\n {output['flow_sim'][:5,:5]}\")  # TODO: Add a visualization here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'RmseLossFlowComb', 'weights': {'w1': 11.0, 'w2': 1.0}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 2. Training a Model\n",
    "\n",
    "Now that we can build a differentiable model, we will train $\\delta$ HBV (1.0 or 1.1p) and expose critical steps in the\n",
    "process, particularly within the model trainer.\n",
    "\n",
    "### 2.1 Load Config and Dataset\n",
    "\n",
    "Once again, we begin by loading in the config dict and CAMELS dataset.\n",
    "\n",
    "Be sure to change `CONFIG_PATH` to the HBV version you want to use.\n",
    "\n",
    "<!-- This code instantiates a model Trainer which will train or test a model per the user's specification in the config. Note\n",
    "that `__main__.py` is trimmed-down here to illustrate it's primary objective. The trainer performs the following\n",
    "functions:\n",
    "- CAMELS data will be loaded and preprocessed,\n",
    "- A differenial model object with the HBV1.1p backbone will be created, and \n",
    "- An optimizer and loss function will be initialized. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')  # Add the root directory of deltaModel\n",
    "\n",
    "from example import load_config \n",
    "from deltaModel.core.data.dataset_loading import get_dataset_dict # Eventually a hydroData import\n",
    "\n",
    "\n",
    "\n",
    "CONFIG_PATH = '../conf/dhbv_config.yaml'\n",
    "\n",
    "\n",
    "\n",
    "# Load configuration dictionary of model parameters and options\n",
    "config = load_config(CONFIG_PATH)\n",
    "\n",
    "# Setup a dataset dictionary of NN and physics model inputs.\n",
    "# Take a sample to reduce size on GPU.\n",
    "dataset = get_dataset_dict(config, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Initialize a $\\delta$ HBV Differentiable Model, Optimizer, and Loss Function\n",
    "\n",
    "These are the auxillary tasks completed by the Trainer before beginning the training loop.\n",
    "\n",
    "\n",
    "<!-- We use the Adadelta optimizer from PyTorch, feeding it both learnable model\n",
    "parameters and a learning rate from the config file.\n",
    "\n",
    "\n",
    "Dynamically load the loss function identified in the config (RMSE for\n",
    "dHBV 1.0 and NSE for dHBV 1.1p). -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is our dHBV framework: \n",
      " ----- \n",
      " DeltaModel(\n",
      "  (phy_model): HBVMulTDET()\n",
      "  (nn_model): CudnnLstmModel(\n",
      "    (linearIn): Linear(in_features=38, out_features=256, bias=True)\n",
      "    (lstm): CudnnLstm()\n",
      "    (linearOut): Linear(in_features=256, out_features=210, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from hydroDL2.models.hbv.hbv import HBVMulTDET as hbv\n",
    "from deltaModel.models.loss_functions import get_loss_fn\n",
    "from deltaModel.models.neural_networks import init_nn_model\n",
    "from deltaModel.models.differentiable_model import DeltaModel as dHBV\n",
    "\n",
    "\n",
    "\n",
    "# Initialize physical model and neural network\n",
    "phy_model = hbv(config['dpl_model'])\n",
    "nn = init_nn_model(phy_model, config['dpl_model'])\n",
    "\n",
    "# Create the differentiable model dHBV: \n",
    "# a torch.nn.Module that describes how nn is linked to the physical model.\n",
    "dpl_model = dHBV(phy_model, nn)\n",
    "print(f\"Here is our dHBV framework: \\n ----- \\n {dpl_model}\")\n",
    "\n",
    "# Init an Adadelta optimizer\n",
    "optimizer = torch.optim.Adadelta(\n",
    "    dpl_model.parameters(),\n",
    "    lr=config['dpl_model']['nn_model']['learning_rate']\n",
    "    )\n",
    "\n",
    "# init a loss function\n",
    "loss_fn = get_loss_fn(config, dataset['obs'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Train the Model\n",
    "\n",
    "Below we use a basic training loop to train the LSTM in $\\delta$ HBV to optimize HBV's parameters and improve its\n",
    "streamflow predictions.\n",
    "\n",
    "\n",
    "#### Key Steps in the Training Loop\n",
    "1. **Calculate Training Parameters**  \n",
    "   The `calc_training_params` function calculates the training settings:\n",
    "   - `n_sites`: The number of unique locations/sites in the dataset.\n",
    "   - `n_minibatch`: The number of samples to process per epoch.\n",
    "   - `n_timesteps`: The number of timesteps per sample.\n",
    "\n",
    "2. **Epoch Loop**  \n",
    "   Each epoch represents one full cycle through the training data. For each epoch:\n",
    "   - `total_loss` is reset to track the total error across all batches within the epoch.\n",
    "\n",
    "3. **Batch Loop**  \n",
    "   Within each epoch, the code processes data in smaller chunks (minibatches) to improve training efficiency and avoid\n",
    "   oversaturation of GPU VRAM. \n",
    "   \n",
    "   For each batch:\n",
    "   - **Sample Data**: `take_sample_train` randomly selects a sample of training data for the batch.\n",
    "   - **Forward Pass**: The model processes the input data to produce predictions.\n",
    "   - **Calculate Loss**: `loss_fn` compares predictions to observed values to calculate the error for the batch.\n",
    "   - **Backward Pass and Optimization**: \n",
    "     - `loss.backward()` computes gradients to adjust the model’s parameters.\n",
    "     - `optimizer.step()` updates the LSTM parameters.\n",
    "     - `optimizer.zero_grad()` resets gradients for the next batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|          | 0/194 [00:00<?, ?it/s]/data/lgl5139/project_blue_eyes/generic_diffModel/example/differentiable_hydrology/../../deltaModel/models/neural_networks/lstm_models.py:103: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /opt/conda/conda-bld/pytorch_1729647348947/work/aten/src/ATen/native/cudnn/RNN.cpp:1410.)\n",
      "  output, hy, cy, reserve, new_weight_buf = torch._cudnn_rnn(\n",
      "                                                            \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m dataset_sample \u001b[38;5;241m=\u001b[39m take_sample_train(config, dataset, n_sites, n_timesteps)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Forward pass through dPL model.\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mdpl_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_sample\u001b[49m\u001b[43m)\u001b[49m        \n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Calculate loss.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(config,\n\u001b[1;32m     33\u001b[0m                predictions,\n\u001b[1;32m     34\u001b[0m                dataset_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     35\u001b[0m                igrid\u001b[38;5;241m=\u001b[39mdataset_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miGrid\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     36\u001b[0m                )\n",
      "File \u001b[0;32m/data/lgl5139/project_blue_eyes/generic_diffModel/example/differentiable_hydrology/../../deltaModel/models/differentiable_model.py:167\u001b[0m, in \u001b[0;36mDeltaModel.forward\u001b[0;34m(self, data_dict)\u001b[0m\n\u001b[1;32m    164\u001b[0m params_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbreakdown_params(params_all)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# Physics model\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphy_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx_hydro_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhydro_params_raw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrouting_parameters\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparams_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconv_params_hydro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarm_up_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarm_up_mode\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# Baseflow index percentage; (from Farshid)\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# Using two deep groundwater buckets: gwflow & bas_shallow\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbas_shallow\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m predictions\u001b[38;5;241m.\u001b[39mkeys():\n",
      "File \u001b[0;32m/data/lgl5139/.conda/envs/hydrodl/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/lgl5139/.conda/envs/hydrodl/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/data/lgl5139/project_blue_eyes/hydroDL2/src/hydroDL2/models/hbv/hbv.py:205\u001b[0m, in \u001b[0;36mHBVMulTDET.forward\u001b[0;34m(self, x, parameters, muwts, comprout, routing_parameters, warm_up_mode)\u001b[0m\n\u001b[1;32m    203\u001b[0m ETact \u001b[38;5;241m=\u001b[39m PETm[t, :, :] \u001b[38;5;241m*\u001b[39m evapfactor\n\u001b[1;32m    204\u001b[0m ETact \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmin(SM, ETact)\n\u001b[0;32m--> 205\u001b[0m SM \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclamp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSM\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mETact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnearzero\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# SM != 0 for grad tracking.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# Groundwater boxes -------------------------------\u001b[39;00m\n\u001b[1;32m    208\u001b[0m SUZ \u001b[38;5;241m=\u001b[39m SUZ \u001b[38;5;241m+\u001b[39m recharge \u001b[38;5;241m+\u001b[39m excess\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "from core.data import calc_training_params, take_sample_train\n",
    "from core.utils import save_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Batch size, number of training samples per epoch, and number of timesteps\n",
    "n_sites, n_minibatch, n_timesteps = calc_training_params(\n",
    "    dataset['inputs_nn_scaled'],\n",
    "    config['train_t_range'],\n",
    "    config\n",
    "    )\n",
    "\n",
    "# Start of training.\n",
    "for epoch in range(1, config['train']['epochs'] + 1):\n",
    "    total_loss = 0.0 # Initialize epoch loss to zero.\n",
    "\n",
    "    # Work through training data in batches.\n",
    "    prog_str = f\"Epoch {epoch}/{config['train']['epochs']}\"\n",
    "    \n",
    "    for i in tqdm.tqdm(range(1, n_minibatch + 1), desc=prog_str,\n",
    "                       leave=False, dynamic_ncols=True):\n",
    "        \n",
    "        # Take a sample of the training data for the batch.\n",
    "        dataset_sample = take_sample_train(config, dataset, n_sites, n_timesteps)\n",
    "\n",
    "        # Forward pass through dPL model.\n",
    "        predictions = dpl_model.forward(dataset_sample)        \n",
    "        \n",
    "        # Calculate loss.\n",
    "        loss = loss_fn(config,\n",
    "                       predictions,\n",
    "                       dataset_sample['obs'],\n",
    "                       igrid=dataset_sample['iGrid']\n",
    "                       )\n",
    "                                   \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / n_minibatch + 1\n",
    "    print(f\"Avg model loss after epoch {epoch}: {avg_loss}\")\n",
    "\n",
    "    # Save the model every save_epoch (set in the config).\n",
    "    model_name = config['dpl_model']['phy_model']['model_name']\n",
    "    if epoch % config['train']['save_epoch'] == 0:\n",
    "        save_model(config, dpl_model, model_name, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 3 Test a Model\n",
    "\n",
    "If you have run testing on a trained model and want to view the results, you can find a `mstd.csv` file in your\n",
    "`results/` directory, which will present performance statistics for the respective model. \n",
    "\n",
    "*A testing tutorial and Graphical visualizations of model output will be supported in a future updated.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydrodl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
