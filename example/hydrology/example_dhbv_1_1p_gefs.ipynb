{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tutorial: **δHBV 1.1p** (GEFS)\n",
    "\n",
    "---\n",
    "\n",
    "This notebook demonstrates training and forward simulation with the δHBV 1.1p model developed by [Yalan Song et al. (2025)](https://doi.org/10.22541/essoar.172304428.82707157/v2). A pre-trained model is provided for those who only wish to run the model forward.\n",
    "\n",
    "For explanation of model structure, methodologies, data, and performance metrics, please refer to Song's publication [below](#publication). If you find this code is useful in your own work, please include the aforementioned citation.\n",
    "\n",
    "**Note**: If you are new to the dMG framework, we suggest first looking at our [δHBV 1.0 tutorial](./../hydrology/example_dhbv.ipynb).\n",
    "\n",
    "<br>\n",
    "\n",
    "### Before Running:\n",
    "- **Environment**: See [setup.md](./../../docs/setup.md) for ENV setup. dMG must be installed with dependencies + hydrodl2 to run this notebook.\n",
    "\n",
    "- **Model**: Download pretrained δHBV 1.1p model weights from [AWS](https://mhpi-spatial.s3.us-east-2.amazonaws.com/mhpi-release/models/dhbv_1_1p_trained.zip). Then update the model config:\n",
    "\n",
    "    - In [`./generic_deltamodel/example/conf/config_dhbv_1_1p.yaml`](./../conf/config_dhbv_1_1p.yaml), update *trained_model* with your path to the parent directory containing both trained model weights `dhbv_1_1p_ep50.pt` (or *Ep100*) **and** normalization file `normalization_statistics.json`.\n",
    "    - **Note**: make sure this path includes the last closing forward slash: e.g., `./your/path/to/model/`.\n",
    "\n",
    "- **Data**: Download the CAMELS data extraction from [AWS](https://mhpi-spatial.s3.us-east-2.amazonaws.com/mhpi-release/camels/camels_data.zip). Then, updated the data configs:\n",
    "\n",
    "    - In [`./generic_deltamodel/example/conf/observations/camels_531.yaml`](./../conf/observations/camels_531.yaml) and [`camels_671.yaml`](./../conf/observations/camels_671.yaml), update...\n",
    "        1. *data_path* with `camels_dataset` path,\n",
    "        2. *gage_info* with `gage_ids.npy` path,\n",
    "        3. *subset_path* with `531_subset.txt` path (camels_531 only).\n",
    "\n",
    "    - The full 671-basin or 531-basin CAMELS datasets can be selected by setting `observations: camels_671` or `camels_531` in the model config, respectively.\n",
    "\n",
    "- **Hardware**: The NNs used in this model require CUDA support only available with Nvidia GPUs. For those without access, T4 GPUs can be used when running this notebook with dMG on [Google Colab](https://colab.research.google.com/).\n",
    "\n",
    "<br>\n",
    "\n",
    "### Publication:\n",
    "\n",
    "*Yalan Song, Kamlesh Sawadekar, Jonathan M Frame, Ming Pan, Martyn Clark, Wouter J M Knoben, Andrew W Wood, Trupesh Patel, Chaopeng Shen. \"Physics-informed, Differentiable Hydrologic  Models for Capturing Unseen Extreme Events.\" ESS Open Archive (2025). https://doi.org/10.22541/essoar.172304428.82707157/v2.*\n",
    "\n",
    "<br>\n",
    "\n",
    "### Issues:\n",
    "For questions, concerns, bugs, etc., please reach out by posting an [issue](https://github.com/mhpi/generic_deltamodel/issues).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "## 1. Forward δHBV 1.1p\n",
    "\n",
    "After completing [these](#before-running) steps, forward δHBV 1.1p with the code block below.\n",
    "\n",
    "Note:\n",
    "- The settings defined in the config `./generic_deltamodel/example/conf/config_dhbv_1_1p.yaml` are set to replicate benchmark performance on 531 CAMELS basins.\n",
    "- The first year (`warm_up` in the config, default is 365 days) of the inference period is used for initializing HBV's internal states (water storages) and is, therefore, excluded from the model's prediction output.\n",
    "\n",
    "### 1.1 Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../../')\n",
    "from example import load_config\n",
    "from src.dmg import ModelHandler\n",
    "from src.dmg.core.utils import import_data_loader, print_config, set_randomseed\n",
    "\n",
    "from ._gefs_utils import print_dataset_info\n",
    "\n",
    "# ------------------------------------------#\n",
    "# Define model settings here.\n",
    "CONFIG_PATH = '../example/conf/config_dhbv_1_1p.yaml'\n",
    "# ------------------------------------------#\n",
    "\n",
    "# 1. Load configuration dictionary of model parameters and options.\n",
    "config = load_config(CONFIG_PATH)\n",
    "config['mode'] = 'simulation'\n",
    "print_config(config)\n",
    "print(\"Config Keys:\", config.keys())\n",
    "\n",
    "# Set random seed for reproducibility.\n",
    "set_randomseed(config['random_seed'])\n",
    "\n",
    "# 2. Initialize the differentiable HBV 1.1p model (LSTM + HBV 1.1p).\n",
    "model = ModelHandler(config, verbose=True)  # model key is ['Hbv_1_1p']\n",
    "nn_model = model.model_dict[\"Hbv_1_1p\"].nn_model\n",
    "print(\"NN model class:\", type(nn_model).__name__)\n",
    "print(\"Model Keys:\", model.model_dict.keys())\n",
    "print(\"Model Keys:\", model.model_dict['Hbv_1_1p'])\n",
    "\n",
    "# 3. Load and initialize a dataset dictionary of NN and HBV model inputs.\n",
    "data_loader_cls = import_data_loader(config['data_loader'])\n",
    "data_loader = data_loader_cls(config, test_split=False, overwrite=False)\n",
    "print_dataset_info(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define Core GEFS Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from ._gefs_utils import GEFSdataErrorCheck, to_time_first\n",
    "\n",
    "\n",
    "def compute_bias_correction_from_dataset(\n",
    "    daymet_tensor,\n",
    "    gefs_df,\n",
    "    timesteps,\n",
    "    basin_idx,\n",
    "    window=15,\n",
    "    method=\"scalar\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute bias correction factors (scalar or CDF-based) for GEFS forcings\n",
    "    using Daymet climatology in a moving DOY window.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    daymet_tensor : torch.Tensor [time, basin, 3]\n",
    "        Historical Daymet forcings from the dataset.\n",
    "    gefs_df : pd.DataFrame\n",
    "        GEFS historical forcings with columns: date, prcp, tmean, pet.\n",
    "    timesteps : np.ndarray\n",
    "        Daily time axis corresponding to Daymet.\n",
    "    basin_idx : int\n",
    "        Basin index to select from Daymet tensor.\n",
    "    window : int\n",
    "        ±days around each DOY to compute moving-window stats.\n",
    "    method : str, {\"scalar\", \"cdf\"}\n",
    "        Bias correction method:\n",
    "        - \"scalar\": mean ratio (current method)\n",
    "        - \"cdf\": quantile mapping / CDF matching\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary of corrections per variable.\n",
    "        For \"scalar\" → corrections[var][doy] = scalar value\n",
    "        For \"cdf\" → corrections[var][doy] = (p_grid, mapped_values)\n",
    "    \"\"\"\n",
    "    # Convert Daymet tensor → DataFrame\n",
    "    df_daymet = pd.DataFrame(\n",
    "        {\n",
    "            \"date\": pd.to_datetime(timesteps),\n",
    "            \"prcp\": daymet_tensor[:, basin_idx, 0].detach().cpu().numpy(),\n",
    "            \"tmean\": daymet_tensor[:, basin_idx, 1].detach().cpu().numpy(),\n",
    "            \"pet\": daymet_tensor[:, basin_idx, 2].detach().cpu().numpy(),\n",
    "        },\n",
    "    )\n",
    "    df_daymet[\"doy\"] = df_daymet[\"date\"].dt.dayofyear\n",
    "\n",
    "    gefs_df = gefs_df.copy()\n",
    "    gefs_df[\"doy\"] = gefs_df[\"date\"].dt.dayofyear\n",
    "\n",
    "    corrections = {v: {} for v in [\"prcp\", \"tmean\", \"pet\"]}\n",
    "\n",
    "    for var in [\"prcp\", \"tmean\", \"pet\"]:\n",
    "        for doy in range(1, 366):\n",
    "            mask_d = (df_daymet[\"doy\"] - doy).abs() <= window\n",
    "            mask_g = (gefs_df[\"doy\"] - doy).abs() <= window\n",
    "\n",
    "            vals_d = df_daymet.loc[mask_d, var].dropna()\n",
    "            vals_g = gefs_df.loc[mask_g, var].dropna()\n",
    "\n",
    "            if len(vals_d) < 10 or len(vals_g) < 10:\n",
    "                continue\n",
    "\n",
    "            if method == \"scalar\":\n",
    "                mean_d = vals_d.mean()\n",
    "                mean_g = vals_g.mean()\n",
    "                corrections[var][doy] = mean_d / mean_g if mean_g > 1e-6 else 1.0\n",
    "\n",
    "            elif method == \"cdf\":\n",
    "                # Create percentile grid (0–100)\n",
    "                p_grid = np.linspace(0, 100, 101)\n",
    "                q_daymet = np.percentile(vals_d, p_grid)\n",
    "                q_gefs = np.percentile(vals_g, p_grid)\n",
    "                corrections[var][doy] = (q_gefs, q_daymet)  # (x_in, x_out)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown correction method '{method}'\")\n",
    "\n",
    "    return corrections\n",
    "\n",
    "\n",
    "def pre_processing(df, corrections, method=\"scalar\"):\n",
    "    \"\"\"Apply bias correction (scalar or CDF) to GEFS forecast DataFrame.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"doy\"] = df[\"date\"].dt.dayofyear\n",
    "\n",
    "    for var in [\"prcp\", \"tmean\", \"pet\"]:\n",
    "        if method == \"scalar\":\n",
    "            df[var] *= df[\"doy\"].map(lambda d, var=var: corrections[var].get(d, 1.0))\n",
    "        elif method == \"cdf\":\n",
    "            corrected_vals = []\n",
    "            for _, row in df.iterrows():\n",
    "                doy = row[\"doy\"]\n",
    "                val = row[var]\n",
    "                if doy not in corrections[var]:\n",
    "                    corrected_vals.append(val)\n",
    "                    continue\n",
    "                q_in, q_out = corrections[var][doy]\n",
    "                # interpolate within empirical CDFs\n",
    "                corrected_vals.append(np.interp(val, q_in, q_out))\n",
    "            df[var] = corrected_vals\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown correction method '{method}'\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def post_processing(sim_pre_GEFS, ens_preds):  # post processing\n",
    "    \"\"\"\n",
    "    Apply a constant offset correction so that the first forecasted streamflow\n",
    "    connects smoothly with the last simulated (Daymet) streamflow.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sim_pre_GEFS : np.ndarray\n",
    "        1D array of simulated streamflow before the forecast period (from HBV using Daymet).\n",
    "    ens_preds : np.ndarray\n",
    "        2D array of ensemble forecasts with shape (N_ENSEMBLES, FORECAST_LENGTH).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Offset-corrected ensemble forecasts of the same shape.\n",
    "    \"\"\"\n",
    "    Q_sim_end = sim_pre_GEFS[-1]  # last simulated streamflow value\n",
    "    ens_preds_corrected = []\n",
    "\n",
    "    for ens in ens_preds:\n",
    "        offset = Q_sim_end - ens[0]  # compute offset\n",
    "        ens_corrected = np.maximum(ens + offset, 0.0)  # apply and ensure non-negative\n",
    "        ens_preds_corrected.append(ens_corrected)\n",
    "\n",
    "    return np.array(ens_preds_corrected)\n",
    "\n",
    "\n",
    "def run_warm_forecasts(\n",
    "    hbv,\n",
    "    pars_last,\n",
    "    rtwts_hist,\n",
    "    warm_states,\n",
    "    gage_id,\n",
    "    start_date,\n",
    "    horizon,\n",
    "    varF,\n",
    "    N_ENSEMBLES,\n",
    "    GEFS_DIR,\n",
    "    timesteps,\n",
    "    WINDOW,\n",
    "    CORRECTION,\n",
    "    device,\n",
    "):\n",
    "    \"\"\"Run warm-started HBV forecasts for all GEFS ensembles.\"\"\"\n",
    "    ens_preds = []\n",
    "    # will store per-basin correction once\n",
    "\n",
    "    for ens_id in range(N_ENSEMBLES):\n",
    "        # ----- Load GEFS forecast for this ensemble -----\n",
    "        f_path = os.path.join(GEFS_DIR, f\"ens0{ens_id + 1}\", f\"{gage_id:08d}.txt\")\n",
    "        if not os.path.exists(f_path):\n",
    "            raise ValueError(f\"Missing GEFS file: {f_path}\")\n",
    "\n",
    "        df = pd.read_csv(f_path, sep=r\"\\s+\", header=0)\n",
    "        df = df.rename(\n",
    "            columns={\n",
    "                \"Year\": \"year\",\n",
    "                \"Mnth\": \"month\",\n",
    "                \"Day\": \"day\",\n",
    "                \"prcp(mm/day)\": \"prcp\",\n",
    "                \"tmean(C)\": \"tmean\",\n",
    "                \"pet(mm/day)\": \"pet\",\n",
    "            },\n",
    "        )\n",
    "        df[\"date\"] = pd.to_datetime(df[[\"year\", \"month\", \"day\"]])\n",
    "\n",
    "        # ----- Extract forecast block -----\n",
    "        idx_list = df.index[df[\"date\"] == start_date].to_list()\n",
    "        if not idx_list:\n",
    "            raise ValueError(f\"Start date {start_date} not found in {f_path}\")\n",
    "        fc_block = df.iloc[idx_list[0] : idx_list[0] + horizon]\n",
    "        GEFSdataErrorCheck(idx_list, fc_block, horizon, start_date)\n",
    "\n",
    "        # ----- Compute bias correction once (for the first ensemble) -----\n",
    "        if not CORRECTION:\n",
    "            bias_corrections = compute_bias_correction_from_dataset(\n",
    "                dataset[\"x_phy\"],\n",
    "                df,\n",
    "                timesteps,\n",
    "                basin_idx,\n",
    "                window=WINDOW,\n",
    "                method=CORRECTION,  # or \"scalar\"\n",
    "            )\n",
    "            print(f\"Bias correction computed for basin {gage_id}\")\n",
    "            # ----- Apply bias correction -----\n",
    "            fc_block = pre_processing(fc_block, bias_corrections, method=CORRECTION)\n",
    "\n",
    "        # ----- Convert forcings to tensor -----\n",
    "        raw_np = fc_block[varF].to_numpy().astype(np.float32)\n",
    "        forc_raw = torch.tensor(\n",
    "            raw_np[np.newaxis, :, :],\n",
    "            dtype=torch.float32,\n",
    "            device=device,\n",
    "        )\n",
    "        forc_raw[torch.isnan(forc_raw)] = 0.0\n",
    "\n",
    "        # ----- Run HBV forecast with warm states -----\n",
    "        with torch.no_grad():\n",
    "            Qs_fc = hbv(\n",
    "                x=to_time_first(forc_raw),\n",
    "                parameters=pars_last,\n",
    "                staind=staind,\n",
    "                tdlst=tdRep,\n",
    "                mu=nmul,\n",
    "                muwts=None,\n",
    "                rtwts=rtwts_hist,\n",
    "                bufftime=0,\n",
    "                outstate=False,\n",
    "                instate=True,\n",
    "                init_states=warm_states,\n",
    "                routOpt=routing,\n",
    "                dydrop=dydrop,\n",
    "            )\n",
    "\n",
    "        # ----- Sanity check and store -----\n",
    "        ens_fc = Qs_fc[:, 0, 0].detach().cpu().numpy()\n",
    "        if np.isnan(ens_fc).all():\n",
    "            raise ValueError(\n",
    "                f\"ens_fc are all NaNs for basin {gage_id}, ensemble {ens_id}!\",\n",
    "            )\n",
    "\n",
    "        ens_preds.append(ens_fc)\n",
    "\n",
    "    return np.array(ens_preds)  # shape: (N_ENSEMBLES, horizon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 GEFS Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from hydrodl2.models.hbv import HBV1_1p as h1pp\n",
    "\n",
    "from ._gefs_utils import (\n",
    "    get_parameters_from_model,\n",
    "    plot_ensemble_hydrograph,\n",
    "    selectbasins,\n",
    "    startid_endid,\n",
    ")\n",
    "\n",
    "# ---------------- Simulation settings ---------------- #\n",
    "START_DATE = pd.to_datetime(\"2014-09-16\")\n",
    "FORECAST = 15\n",
    "N_ENSEMBLES = 5\n",
    "N_BASINS = 5  # randomly pick basins\n",
    "SEED = 1234  # config[\"random_seed\"]\n",
    "RANDOM = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "GEFS_DIR = \"/u/st/dr/awwood/aw-ciroh-proj/projects/dl_da/daymet-gefs-camels-gII\"\n",
    "GAGE_NAME_PATH = \"/beegfs/projects/aw-ciroh/common/datasets/camels/CAMELS_US/camels_attributes_v2.0/camels_name.txt\"\n",
    "dataset = data_loader.dataset\n",
    "WINDOW = 30  # pre-processing on correcting window (in days)\n",
    "CORRECTION = 'scalar'  #  # or 'cdf'\n",
    "# ----------------------------------------------------- #\n",
    "\n",
    "\n",
    "# HBV options\n",
    "staind = -1\n",
    "tdRep = [1, 3, 13]\n",
    "VAR_F = config['delta_model']['phy_model']['forcings']\n",
    "BUFFTIME = config['delta_model']['phy_model']['warm_up']\n",
    "routing = config['delta_model']['phy_model']['routing']\n",
    "dydrop = config['delta_model']['phy_model']['dy_drop']\n",
    "nmul = config[\"delta_model\"][\"phy_model\"][\"nmul\"]\n",
    "\n",
    "# Plot settings\n",
    "history_len = 120\n",
    "\n",
    "\n",
    "# ---------------- Obtain basin IDs ---------------- #\n",
    "gage_ids = np.load(config[\"observations\"][\"gage_info\"], allow_pickle=True)\n",
    "subset_file = config[\"observations\"][\"subset_path\"]\n",
    "with open(subset_file) as f:\n",
    "    content = f.read().strip()\n",
    "gage_ids_subset = (\n",
    "    json.loads(content)\n",
    "    if content.startswith(\"[\")\n",
    "    else np.loadtxt(subset_file, dtype=int).tolist()\n",
    ")\n",
    "\n",
    "if config[\"observations\"][\"name\"] == \"camels_671\":\n",
    "    basin_pool = list(gage_ids)\n",
    "elif config[\"observations\"][\"name\"] == \"camels_531\":\n",
    "    basin_pool = list(gage_ids_subset)\n",
    "\n",
    "\n",
    "# ---------------- Select 5 random basins ---------------- #\n",
    "selected_basins = selectbasins(RANDOM, SEED, basin_pool, N_BASINS)\n",
    "sidx, eidx, timesteps = startid_endid(START_DATE, FORECAST, config)\n",
    "hbv = h1pp.HBV().to(device)\n",
    "\n",
    "\n",
    "# ---------------- Loop over selected basins ---------------- #\n",
    "for GAGE_ID in selected_basins:\n",
    "    print(f\"\\n=== Running forecast for basin {GAGE_ID} ===\")\n",
    "    basin_idx = basin_pool.index(GAGE_ID)\n",
    "\n",
    "    # ---------------- Get inputs of all forcings 38 from trained NN ---------------- #\n",
    "    hist_dict = {\n",
    "        \"xc_nn_norm\": dataset[\"xc_nn_norm\"][\n",
    "            :sidx,\n",
    "            basin_idx : basin_idx + 1,\n",
    "            :,\n",
    "        ].clone(),\n",
    "    }  # [715, 1, 38] 2years - 15 days = 715 days\n",
    "    print(f\"hist_dict['xc_nn_norm'] shape: {hist_dict['xc_nn_norm'].shape}\")\n",
    "\n",
    "    # ---------------- Get parameters from trained NN ---------------- #\n",
    "    pars_hist, rtwts_hist = get_parameters_from_model(\n",
    "        model.model_dict[\"Hbv_1_1p\"],\n",
    "        hist_dict,\n",
    "        n_par=14,\n",
    "        mu=nmul,\n",
    "        device=device,\n",
    "    )  # pars: 715, 1, 14, 16    rts: 1, 2\n",
    "\n",
    "    pars_first = pars_hist[:1].repeat(sidx, 1, 1, 1)\n",
    "    pars_last = pars_hist[-1:].repeat(FORECAST, 1, 1, 1)\n",
    "\n",
    "    # --- Warm-up HBV with RAW forcings (Daymet) ---\n",
    "    with torch.no_grad():\n",
    "        forc_hist = torch.nan_to_num(\n",
    "            dataset[\"x_phy\"][:sidx, basin_idx : basin_idx + 1, :3],\n",
    "            nan=0.0,\n",
    "        )\n",
    "        Qs_hist, sp, mw, sm, suz, slz = hbv(\n",
    "            x=forc_hist,\n",
    "            parameters=pars_first,\n",
    "            staind=staind,\n",
    "            tdlst=tdRep,\n",
    "            mu=nmul,\n",
    "            muwts=None,\n",
    "            rtwts=rtwts_hist,\n",
    "            bufftime=BUFFTIME,\n",
    "            outstate=True,\n",
    "            instate=False,\n",
    "            routOpt=routing,\n",
    "            dydrop=dydrop,\n",
    "        )\n",
    "    warm_states = (sp, mw, sm, suz, slz)  # checknans(warm_states)\n",
    "\n",
    "    # ---------------- Run ensembles ---------------- #\n",
    "    ens_preds = run_warm_forecasts(\n",
    "        hbv,\n",
    "        pars_last,\n",
    "        rtwts_hist,\n",
    "        warm_states,\n",
    "        GAGE_ID,\n",
    "        START_DATE,\n",
    "        FORECAST,\n",
    "        VAR_F,\n",
    "        N_ENSEMBLES,\n",
    "        GEFS_DIR,\n",
    "        timesteps,\n",
    "        WINDOW,\n",
    "        CORRECTION,\n",
    "        device,\n",
    "    )\n",
    "\n",
    "    # ---------------- Plotting ---------------- #\n",
    "    obs_full_window = (\n",
    "        dataset[\"target\"][sidx - history_len : eidx, basin_idx, 0]\n",
    "        .detach()\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "\n",
    "    Qs_full = Qs_hist[:, 0, 0].detach().cpu().numpy()\n",
    "    sim_pre_GEFS = Qs_full[-(history_len + 1) :]\n",
    "    ens_preds = post_processing(sim_pre_GEFS, ens_preds)  # post-processing\n",
    "\n",
    "    save_path = os.path.join(\n",
    "        \"figs\",\n",
    "        f\"GEFS_{GAGE_ID}\",\n",
    "        f\"GEFS_{GAGE_ID}_{CORRECTION}_w{WINDOW}.png\",\n",
    "    )\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    plot_ensemble_hydrograph(\n",
    "        gage_path=GAGE_NAME_PATH,\n",
    "        gage_id=GAGE_ID,\n",
    "        start_date=START_DATE,\n",
    "        obs=obs_full_window,\n",
    "        sim=sim_pre_GEFS,\n",
    "        ens_preds=ens_preds,\n",
    "        history_len=history_len,\n",
    "        save_path=save_path,\n",
    "    )\n",
    "    print(f\"Saved plot for basin {GAGE_ID} → {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If you want to simulate on the whole range using daymet (nothing to do with GEFS)\n",
    "output = model(\n",
    "    data_loader.dataset,\n",
    "    eval=True,\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"-------------\\n\",\n",
    ")  # only after warmup, only output['Hbv_1_1p']['streamflow'] is meaningful\n",
    "print(\n",
    "    f\"Streamflow predictions for {output['Hbv_1_1p']['streamflow'].shape[0]} days (after warmup) and \"\n",
    "    f\"{output['Hbv_1_1p']['streamflow'].shape[1]} basins ~ \\nShowing the first 5 days for \"\n",
    "    f\"first basin: \\n {output['Hbv_1_1p']['streamflow'][:5, :1].cpu().detach().numpy().squeeze()}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.4 Visualizing Model Predictions Only\n",
    "\n",
    "After running model inference we can, e.g., view the hydrograph for one of the basins to see we are getting expected outputs.\n",
    "\n",
    "We can do this with our target variable, streamflow, for instance (though, there are many other states and fluxes we can view -- see cell output below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.dmg.core.post import plot_hydrograph\n",
    "from src.dmg.core.utils import Dates\n",
    "\n",
    "from ._gefs_utils import obtain_gage_name\n",
    "\n",
    "# ------------------------------------------#\n",
    "# Choose a basin by USGS gage ID to plot.\n",
    "GAGE_ID = 1022500\n",
    "\n",
    "# Resample to 3-day prediction. Options: 'D', 'W', 'M', 'Y'.\n",
    "RESAMPLE = '3D'\n",
    "# ------------------------------------------#\n",
    "\n",
    "gage_ids_531 = np.load('./gage_ids_531.npy', allow_pickle=True)\n",
    "\n",
    "\n",
    "# 1. Get the streamflow predictions and daily timesteps of the prediction window.\n",
    "pred = output['Hbv_1_1p']['streamflow']\n",
    "timesteps = Dates(\n",
    "    config['simulation'],\n",
    "    config['delta_model']['rho'],\n",
    ").batch_daily_time_range\n",
    "timesteps = timesteps[\n",
    "    config['delta_model']['phy_model']['warm_up'] :\n",
    "]  # remove warm-up period\n",
    "if config['observations']['name'] == 'camels_671':\n",
    "    basin_idx = list(gage_ids).index(GAGE_ID) if GAGE_ID in gage_ids else None\n",
    "elif config['observations']['name'] == 'camels_531':\n",
    "    basin_idx = (\n",
    "        list(gage_ids_531).index(GAGE_ID) if GAGE_ID in gage_ids_subset else None\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported dataset: {config['observations']}\")\n",
    "\n",
    "# 2. Get the data for the chosen basin and plot.\n",
    "streamflow_pred_basin = pred[:, basin_idx].squeeze()\n",
    "gage_name = obtain_gage_name(GAGE_NAME_PATH, GAGE_ID)\n",
    "plot_hydrograph(\n",
    "    timesteps,\n",
    "    streamflow_pred_basin,\n",
    "    resample=RESAMPLE,\n",
    "    title=f\"Hydrograph for Gage ID {GAGE_ID} ({gage_name})\",\n",
    "    ylabel='Streamflow (mm/day)',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "## 2. Train δHBV 1.1p \n",
    "\n",
    "- Train δHBV 1.1p with the code block below.\n",
    "- If you have already trained data, skip this.\n",
    "- If you have not trained data nor download pretrained data, you won't be able to accomplish Step 1!\n",
    "\n",
    "**Note**\n",
    "- The settings defined in the config `./generic_deltamodel/example/conf/config_dhbv_1_1p.yaml` are set to replicate benchmark performance.\n",
    "- For model training, set `mode: train` in the config, or modify after config dict has been created (see below).\n",
    "- An `./example/generic_deltamodel/output/` directory will be generated to store experiment and model files. This location can be adjusted by changing the *save_path* key in your config. \n",
    "- Default settings with 50 epochs, batch size of 100, and training window from 1 October 1999 to 30 September 2008 should use ~2.8GB of vram. Expect training times of ~9 hours with an Nvidia RTX 3090 Ti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../../')\n",
    "\n",
    "from example import load_config\n",
    "\n",
    "from dmg import ModelHandler\n",
    "from dmg.core.utils import (\n",
    "    import_data_loader,\n",
    "    import_trainer,\n",
    "    print_config,\n",
    "    set_randomseed,\n",
    ")\n",
    "\n",
    "# ------------------------------------------#\n",
    "# Define model settings here.\n",
    "CONFIG_PATH = '../example/conf/config_dhbv_1_1p.yaml'\n",
    "# ------------------------------------------#\n",
    "\n",
    "# 0. zhennan added --- tiny tee: mirror all prints/progress to train.log ---\n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# tqdm writes to stderr by default; redirect to file also\n",
    "logfile = open(\"train_progress.log\", \"a\", buffering=1)\n",
    "\n",
    "\n",
    "class TqdmLogger:\n",
    "    \"\"\"Logger class to redirect tqdm output to multiple files.\"\"\"\n",
    "\n",
    "    def __init__(self, *files):\n",
    "        self.files = files\n",
    "\n",
    "    def write(self, x):\n",
    "        \"\"\"Write string x to all files.\"\"\"\n",
    "        for f in self.files:\n",
    "            f.write(x)\n",
    "            f.flush()\n",
    "\n",
    "    def flush(self):\n",
    "        \"\"\"Flush all files.\"\"\"\n",
    "        for f in self.files:\n",
    "            f.flush()\n",
    "\n",
    "\n",
    "sys.stderr = TqdmLogger(sys.stderr, logfile)  # capture tqdm progress bar\n",
    "\n",
    "\n",
    "# 1. Load configuration dictionary of model parameters and options.\n",
    "config = load_config(CONFIG_PATH)\n",
    "config['mode'] = 'train'\n",
    "print_config(config)\n",
    "\n",
    "# Set random seed for reproducibility.\n",
    "set_randomseed(config['random_seed'])\n",
    "\n",
    "# 2. Initialize the differentiable HBV 1.1p model (LSTM + HBV 1.1p) with model handler.\n",
    "model = ModelHandler(config, verbose=True)\n",
    "\n",
    "# 3. Load and initialize a dataset dictionary of NN and HBV model inputs.\n",
    "data_loader_cls = import_data_loader(config['data_loader'])\n",
    "data_loader = data_loader_cls(config, test_split=True, overwrite=False)\n",
    "\n",
    "\n",
    "# 4. Initialize trainer to handle model training.\n",
    "trainer_cls = import_trainer(config['trainer'])\n",
    "trainer = trainer_cls(\n",
    "    config,\n",
    "    model,\n",
    "    train_dataset=data_loader.train_dataset,\n",
    ")\n",
    "\n",
    "# 5. Start model training.\n",
    "trainer.train()\n",
    "print(f\"Training complete. Model saved to {config['model_path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Evaluate Model Performance\n",
    "\n",
    "After completing the training in [Section 1](#1-train-δhbv-11p), or with the trained model provided, test δHBV 1.1p below on the evaluation data.\n",
    "\n",
    "**Note**\n",
    "- For model evaluation, set `mode: test` in the config, or modify after config dict has been created (see below).\n",
    "- When evaluating provided models, confirm that `test.test_epoch` in the config corresponds the training epochs completed for the model you want to test (e.g., 50 or 100).\n",
    "- Default settings with 50 epochs, batch size of 25, and testing window from 1 October 1989 to 30 September 1999 should use ~2.7GB of VRAM. Expect evalutation times of ~5 minutes with an Nvidia RTX 3090 Ti.\n",
    "\n",
    "### 3.1 Streamflow Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../../')\n",
    "\n",
    "from example import load_config\n",
    "\n",
    "from dmg import ModelHandler\n",
    "from dmg.core.utils import import_data_loader, import_trainer, print_config\n",
    "\n",
    "# ------------------------------------------#\n",
    "# Define model settings here.\n",
    "CONFIG_PATH = '../example/conf/config_dhbv_1_1p.yaml'\n",
    "# ------------------------------------------#\n",
    "\n",
    "\n",
    "# 1. Load configuration dictionary of model parameters and options.\n",
    "config = load_config(CONFIG_PATH)\n",
    "config['mode'] = 'test'\n",
    "print_config(config)\n",
    "\n",
    "set_randomseed(config['random_seed'])\n",
    "\n",
    "# 2. Initialize the differentiable HBV 1.1p model (LSTM + HBV 1.1p).\n",
    "model = ModelHandler(config, verbose=True)\n",
    "\n",
    "# 3. Load and initialize a dataset dictionary of NN and HBV model inputs.\n",
    "data_loader_cls = import_data_loader(config['data_loader'])\n",
    "data_loader = data_loader_cls(config, test_split=True, overwrite=False)\n",
    "\n",
    "# 4. Initialize trainer to handle model evaluation.\n",
    "trainer_cls = import_trainer(config['trainer'])\n",
    "trainer = trainer_cls(\n",
    "    config,\n",
    "    model,\n",
    "    eval_dataset=data_loader.eval_dataset,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# 5. Start testing the model.\n",
    "print('Evaluating model...')\n",
    "trainer.evaluate()\n",
    "print(f\"Metrics and predictions saved to {config['out_path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.2 Visualize Trained Model Performance\n",
    "\n",
    "Once the model has been evaluated, a new directory (e.g., for a model trained for 50 epochs and tested from years 1989-1999), `test1989-1999_ep50/`, will be created in the same directory containing the model files. This path will be populated with...\n",
    "\n",
    "1. All model outputs (fluxes, states), including the target variable, *streamflow* (`streamflow.npy`),\n",
    "\n",
    "2. `streamflow_obs.npy`, streamflow observation data for comparison against model predictions,\n",
    "\n",
    "2. `metrics.json`, containing evaluation metrics accross the test time range for every gage in the dataset,\n",
    "\n",
    "3. `metrics_agg.json`, containing evaluation metric statistics across all sites (mean, median, standard deviation).\n",
    "\n",
    "We can use these outputs to visualize δHBV 1.1p's performance with a \n",
    "1. Cumulative distribution function (CDF) plot, \n",
    "\n",
    "2. CONUS map of gage locations and metric (e.g., NSE) performance.\n",
    "\n",
    "<br>\n",
    "\n",
    "But first, let's first check the (basin-)aggregated metrics for NSE, KGE, bias, RMSE, and, for both high/low flow regimes, RMSE and absolute percent bias..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dmg.core.data import load_json\n",
    "from dmg.core.post import print_metrics\n",
    "\n",
    "print(f\"Evaluation output files saved to: {config['out_path']} \\n\")\n",
    "\n",
    "# 1. Load the basin-aggregated evaluation results.\n",
    "metrics_path = os.path.join(config['out_path'], 'metrics_agg.json')\n",
    "metrics = load_json(metrics_path)\n",
    "print(f\"Available metrics: {metrics.keys()} \\n\")\n",
    "\n",
    "# 2. Print the evaluation results.\n",
    "metric_names = [\n",
    "    # Choose metrics to show.\n",
    "    'nse',\n",
    "    'kge',\n",
    "    'bias',\n",
    "    'rmse',\n",
    "    'rmse_low',\n",
    "    'rmse_high',\n",
    "    'flv_abs',\n",
    "    'fhv_abs',\n",
    "]\n",
    "print_metrics(metrics, metric_names, mode='median', precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 CDF Plot\n",
    "\n",
    "The cumulative distribution function (CDF) plot tells us what percentage (CDF on the y-axis) of basins performed at least better than a given metric on the evaluation data.\n",
    "\n",
    "An example is given below for NSE, but you can change to your preferred metric (see the output from the previous cell), but note some may require changing *xbounds* in `plot_cdf()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dmg.core.post import plot_cdf\n",
    "\n",
    "# ------------------------------------------#\n",
    "# Choose the metric to plot. (See available metrics printed above, or in the metrics_agg.json file).\n",
    "METRIC = 'nse'\n",
    "# ------------------------------------------#\n",
    "\n",
    "\n",
    "# 1. Load the evaluation metrics.\n",
    "metrics_path = os.path.join(config['out_path'], 'metrics.json')\n",
    "metrics = load_json(metrics_path)\n",
    "\n",
    "# 2. Plot the CDF for NSE.\n",
    "plot_cdf(\n",
    "    metrics=[metrics],\n",
    "    metric_names=[METRIC],\n",
    "    model_labels=['δHBV 1.1p'],\n",
    "    title=\"CDF of NSE for δHBV 1.1p\",\n",
    "    xlabel=METRIC.capitalize(),\n",
    "    figsize=(8, 6),\n",
    "    xbounds=(0, 1),\n",
    "    ybounds=(0, 1),\n",
    "    show_arrow=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.4 Spatial Plot\n",
    "\n",
    "This plot shows the locations of each basin in the evaluation data, color-coded by performance on a metric. Here we give a plot for NSE, but as before, this can be changed to your preference. (See above; for metrics not valued between 0 and 1, you will need to set `dynamic_colorbar=True` in `geoplot_single_metric` to ensure proper coding.)\n",
    "\n",
    "Note, you will need to add paths to the CAMELS shapefile, gage IDs, and 531-gage subset which can be found in the [CAMELS download](#before-running)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dmg.core.data import txt_to_array\n",
    "from dmg.core.post import geoplot_single_metric\n",
    "\n",
    "# ------------------------------------------#\n",
    "# Choose the metric to plot. (See available metrics printed above, or in the metrics_agg.json file).\n",
    "METRIC = 'nse'\n",
    "\n",
    "# Set the paths to the gage id lists and shapefiles...\n",
    "GAGE_ID_PATH = config['observations']['gage_info']  # ./gage_id.npy\n",
    "GAGE_ID_531_PATH = config['observations']['subset_path']  # ./531sub_id.txt\n",
    "SHAPEFILE_PATH = '/beegfs/scratch/zhennanshi/DM/generic_deltamodel/extra_files_zhennan/camels_loc/camels_671_loc.shp'  ## zhennan added to his directory\n",
    "\n",
    "# ------------------------------------------#\n",
    "\n",
    "\n",
    "# 1. Load gage ids + basin shapefile with geocoordinates (lat, long) for every gage.\n",
    "gage_ids = np.load(GAGE_ID_PATH, allow_pickle=True)\n",
    "gage_ids_subset = txt_to_array(GAGE_ID_531_PATH)\n",
    "coords = gpd.read_file(SHAPEFILE_PATH)\n",
    "\n",
    "# 2. Format geocoords for 531- and 671-basin CAMELS sets.\n",
    "coords_531 = coords[coords['gage_id'].isin(list(gage_ids_subset))].copy()\n",
    "\n",
    "coords['gage_id'] = pd.Categorical(\n",
    "    coords['gage_id'],\n",
    "    categories=list(gage_ids),\n",
    "    ordered=True,\n",
    ")\n",
    "coords_531['gage_id'] = pd.Categorical(\n",
    "    coords_531['gage_id'],\n",
    "    categories=list(gage_ids_subset),\n",
    "    ordered=True,\n",
    ")\n",
    "\n",
    "coords = coords.sort_values('gage_id')  # Sort to match order of metrics.\n",
    "basin_coords_531 = coords_531.sort_values('gage_id')\n",
    "\n",
    "# 3. Load the evaluation metrics.\n",
    "metrics_path = os.path.join(config['out_path'], 'metrics.json')\n",
    "metrics = load_json(metrics_path)\n",
    "\n",
    "# 4. Add the evaluation metrics to the basin shapefile.\n",
    "if config['observations']['name'] == 'camels_671':\n",
    "    coords[METRIC] = metrics[METRIC]\n",
    "    full_data = coords\n",
    "elif config['observations']['name'] == 'camels_531':\n",
    "    coords_531[METRIC] = metrics[METRIC]\n",
    "    full_data = coords_531\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"Observation data supported: 'camels_671' or 'camels_531'. Got: {config['observations']}\",\n",
    "    )\n",
    "\n",
    "# 5. Plot the evaluation results spatially.\n",
    "geoplot_single_metric(\n",
    "    full_data,\n",
    "    METRIC,\n",
    "    rf\"Spatial Map of {METRIC.upper()} for δHBV 1.1p on CAMELS \"\n",
    "    f\"{config['observations']['name'].split('_')[-1]}\",\n",
    "    dynamic_colorbar=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dmg312)",
   "language": "python",
   "name": "dmg312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
