{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: **$\\delta$ HBV 1.0**\n",
    "\n",
    "---\n",
    "\n",
    "This notebook demonstrates, in detail, how to train and forward the $\\delta$ HBV 1.0 model developed by [Dapeng Feng et al. (2022)](https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2022WR032404). A pre-trained model is provided for those who only wish to run the model forward. For explanation of model structure, methodologies, data, and performance metrics, please refer to Feng's publication [below](#publication). If you find this code is useful in your own work, please include the aforementioned citation.\n",
    "\n",
    "<br>\n",
    "\n",
    "*We include some discussion of differentiable modeling methodology, and recommend this notebook as a starting point for an operational understanding of the dMG framework*\n",
    "\n",
    "<br>\n",
    "\n",
    "### Before Running:\n",
    "- **Environment**: From `env/` a minimal Python environment can be setup for running this code... (see `docs/getting_started.md` for more details.)\n",
    "    - Conda -- `deltamodel_env.yaml`\n",
    "    - Pip -- `requirements.txt`\n",
    "\n",
    "\n",
    "- **Model**: The trained $\\delta$ HBV 1.0 model can be downloaded from [AWS](https://mhpi-spatial.s3.us-east-2.amazonaws.com/mhpi-release/models/dHBV_1_0_trained.zip). After downloading...\n",
    "\n",
    "    - Update the `trained_model` key in model config `example/conf/config_dhbv_1_0.yaml` with the path to you directory containing the trained model `dHBV_1_0_Ep50.pt` (or *Ep100*) AND normalization `test1989-1999_Ep50/normalization_statistics.json`.\n",
    "\n",
    "- **Data**: The CAMELS data extraction used in model training and evaluation can be downloaded from [AWS](https://mhpi-spatial.s3.us-east-2.amazonaws.com/mhpi-release/camels/camels_data.zip). After downloading, in the data configs `example/conf/observations/camels_531.yaml` and `camels_671.yaml` update...\n",
    "    1. `train_path` key with your path to `training_file`,\n",
    "    2. `test_path` with your path to `valication file`,\n",
    "    3. `gage_info` with your path to `gage_ids.npy`,\n",
    "    4. (CAMELS 531 only) `subset_path` with your path to `531_subset.txt`.\n",
    "\n",
    "- **Hardware**: The LSTMs used in this model require CUDA support only available with Nvidia GPUs. For those without access, T4 GPUs can be used when running this notebook with dMG on [Google Colab](https://colab.research.google.com/).\n",
    "\n",
    "\n",
    "\n",
    "### Publication:\n",
    "\n",
    "*Dapeng Feng, Jiangtao Liu, Kathryn Lawson, Chaopeng Shen. \"Differentiable, Learnable, Regionalized Process‐Based Models With Multiphysical Outputs can Approach State‐Of‐The‐Art Hydrologic Prediction Accuracy.\" Water Resources Research 58, no. 10 (2022): e2022WR032404. https://doi.org/10.1029/2022WR032404.*\n",
    "\n",
    "<br>\n",
    "\n",
    "### Issues:\n",
    "For questions, concerns, bugs, etc., please reach out by posting an issue on the [dMG repo](https://github.com/mhpi/generic_deltaModel/issues).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Differentiable Modeling\n",
    "\n",
    "In general, differentiable modeling represents the coupling of a neural network and a (differentiable) physical model. This enables several capabilities, like introducing missing processses and bias corrections. In the applications of these notebooks, we demonstrate parameter learning;\n",
    "\n",
    "Physics models include parameters for which True values are seldom known, but can be approximated with various methods. By coupling a neural network, we can learn a set of a physics model's parameters (static or dynamic in, e.g., time), which can then be passed alongside other input variables to the physical model for making predictions.\n",
    "\n",
    "For $\\delta$ HBV 1.0, we use an LSTM neural network (NN) in concert with the physical model HBV (Beck 2020; Seibert 2005). HBV uses input forcing (time-varying) variables precipitation, temperature, and potential evapotranspiration (PET) accross a collection of hydrologic basins, with physical parameters learned at the same spatiotemporal resolution [timesteps, basins], to make predictions about hydrologic states and fluxes (e.g., streamflow in our case) in both space and time. In general, this differentiable model takes the form\n",
    "\n",
    "$\n",
    "\\delta \\text{HBV} = \n",
    "\\begin{cases}\n",
    "P = \\text{LSTM}(X \\text{, \\ } A) \\\\\n",
    "Q \\text{, \\ } \\mu = \\text{HBV}(X \\text{, \\ } P)\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "where\n",
    "- \\( $P$ \\) the physical parameters used in HBV's equations, learned by a NN;\n",
    "- \\( $X$ \\) represents input weather forcings;\n",
    "- \\( $A$ \\) is the set of attributes belonging to each basin. (This could be any other data that could be related to understanding $P$);\n",
    "- \\( $Q, \\mu$ \\) these are the fluxes and states that the physical model can output.\n",
    "\n",
    "Currently, $\\delta$ HBV is setup to train and make hydrologic predictions on **streamflow**, but it can\n",
    "also be reconfigured without much effort to predict percolation, recharge, and groundwater flow, among others.\n",
    "\n",
    "After showing an example implementation, we'll demonstrate how to train the model and expose critical details of the process.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. Create and Forward $\\delta$ HBV 1.0\n",
    "\n",
    "After completing [these](#before-running) steps (model file not needed), $\\delta$ HBV 1.0 can be built with the code cells [below](#13-demonstration), where we illustrate the model creation process in detail.\n",
    "\n",
    "See section [4](#4-forward--hbv-11p) to see a high-level demonstration of the model forward.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 1.1 Background\n",
    "\n",
    "To create $\\delta$ HBV 1.0 with dMG, we interface with the repository [HydroDL2](https://github.com/mhpi/hydroDL2) containing hydrologic models including those used in these tutorials (see `docs/getting_started.md` for properly setting up this connection). \n",
    "\n",
    "<br>\n",
    "\n",
    "- #### Set Model, Experiment, Dataset Configurations\n",
    "\n",
    "    Two flexible YAML configuration files exist for augmenting behaviors of the dMG framework: One defines model settings and training/testing parameters, and another, defines parameters for your dataset (observations). For this tutorial, two such configuration files have been created and require minimal preparation to use in this notebook:\n",
    "    \n",
    "    1. `/example/conf/config_dhbv_1_0.yaml` -- Model/experiment settings\n",
    "    2. `/example/conf/observations/camels_531.yaml` and `camels_671.yaml` -- CAMELS 531- and 671-basin dataset parameters.\n",
    "\n",
    "    With these, all aspects of dMG model creation, training, testing, etc. can be controlled. As it is, the model/experiment config is setup to use reproduce benchmark results for [$\\delta$ HBV 1.0](#publication) (see [here](https://mhpi.github.io/benchmarks/#10-year-training-comparison)) using the CAMELS 531-basin subset of weather forcings and static basin attributes. Full 671-basin benchmark models can also be trained/tested, and both can be configured by setting the following options in the model config:\n",
    "\n",
    "    - For CAMELS 531-basin, 10-year benchmark (Default):\n",
    "        - `observations: camels_531`\n",
    "        - `train:` 1999/10/01 to 2008/09/30 (`start_time` to `end_time`)\n",
    "        - `test:` 1989/10/01 to 1999/09/30\n",
    "\n",
    "    - For CAMELS 671-basin, 15-year benchmark:\n",
    "        - `observations: camels_671`\n",
    "        - `train:` 1980/10/01 to 1995/09/30 (`start_time` to `end_time`)\n",
    "        - `test:` 1995/10/01 to 2010/09/30\n",
    "\n",
    "<br>\n",
    "\n",
    "- #### Building the Model\n",
    "\n",
    "    There are two ways to build a differentiable model in dMG:\n",
    "    - **Implicit**: Best for small-scale experiments and distribution of final products.\n",
    "\n",
    "        Add/change modules in dMG to create your own differentiable model, and tailor model and dataset configuration files like `deltaModel/conf/config.yaml` and `deltaModel/conf/observations/{dataset}.yaml` to reflect desired behaviors model and experiment behaviors. (Modules like trainers, physics models, neural networks, loss functions, and data loaders/samplers are designed to be hot-swappable per user needs. The differentiable model modality will also be made more flexible to meet diverse modeling needs.)\n",
    "\n",
    "        With these this done, dMG can be run with\n",
    "        ```shell\n",
    "        cd ./deltaModel\n",
    "        python __main__.py\n",
    "        ```\n",
    "\n",
    "    - **Explicit**: Best for exploratory research and prototyping (illustrated in the code block below).\n",
    "\n",
    "        This approach is similar in that we still use config files to hold settings (though\n",
    "        a dictionary object could also be used). The difference is that we are able to expose the fundamental steps in the\n",
    "        modeling process; data preprocesing, model building, and experimentation/forwarding. In doing so, we make it\n",
    "        quicker to develop model and data pipelines, and easier to follow internal processes.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 1.2 Walkthrough\n",
    "\n",
    "The following is an explicit implementation of dMG to create and forward $\\delta$ HBV 1.0:\n",
    "\n",
    "1.  **Load a configuration file**: Using Hydra and OmegaConf packages, we can convert the model/dataset configs into a dictionary object `config`. For example, if your config file contains `mode: train`, the dictionary yields `config['mode'] == 'train'`. However, the config can also contain sub-dictionaries. For instance, \n",
    "    \n",
    "    ```yaml\n",
    "    training: \n",
    "        start_time: 1999/10/01\n",
    "    ```\n",
    "\n",
    "    which is accessed like `config['training']['start_time'] == '1999/10/01'`.\n",
    "\n",
    "2.  **Initialize sub-models**: Next, we initialize the physics model and neural network our differentiable model will use, in this case an LSTM from `deltaModel/models/neural_networks/lstm_models.py` and HBV 1.0 from [HydroDL2](https://github.com/mhpi/hydroDL2).\n",
    "\n",
    "3.  **Load in data**: At this step, we load and process our data as a dictionary of variable and attribute datasets that are used by the neural network and physics model. This dataset_dict is created by a data_loader, and should meet minimum requirements of the base class `deltaModel/core/data/data_loaders/base.py`.\n",
    "\n",
    "    For this example, we take a small, arbitrarily selected sample of the data to illustrate the modeling process.\n",
    "\n",
    "4.  **Create a differentiable model**: Now, the sub-models are linked together by a differentiable model wrapper (`DeltaModel`). This has the effect of interfacing both models to achieve the desired modality, e.g., having the LSTM generate parameters for HBV. \n",
    "\n",
    "5.  **Forward/Experiment**: With the differentiable model created, it can be forwarded (as demonstrated\n",
    "    below), or trained/tested/applied in any user-defined experiments.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 1.3 Demonstration\n",
    "\n",
    "The above steps are demonstrated below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mCurrent Configuration\u001b[0m\n",
      "  Experiment Mode:    predict             \n",
      "  Model 1:            HBV                 \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data Source:        camels_531          \n",
      "  Predict Range :     2012/10/01          2014/09/30          \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Train Epochs:       100                 Batch Size:         100                 \n",
      "  Dropout:            0.5                 Hidden Size:        256                 \n",
      "  Warmup:             365                 Concurrent Models:  16                  \n",
      "  Loss Fn:            RmseLossComb        \n",
      "\n",
      "\u001b[1mMachine\u001b[0m\n",
      "  Use Device:         cuda:0              \n",
      "\n",
      "-------------\n",
      "\n",
      "Streamflow predictions for 365 days and 100 basins:\n",
      "Showing the first 5 days for 5 basins \n",
      " tensor([[[0.0536],\n",
      "         [0.0393],\n",
      "         [0.0344]],\n",
      "\n",
      "        [[0.1276],\n",
      "         [0.0965],\n",
      "         [0.0828]],\n",
      "\n",
      "        [[0.1976],\n",
      "         [0.1517],\n",
      "         [0.1286]]], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../deltaModel')  # Add the root directory of deltaModel\n",
    "\n",
    "from example import load_config, take_data_sample\n",
    "from hydroDL2.models.hbv.hbv import HBV as hbv\n",
    "from deltaModel.models.neural_networks import init_nn_model\n",
    "from deltaModel.models.differentiable_model import DeltaModel as dHBV\n",
    "from core.data.data_loaders.loader_hydro import HydroDataLoader\n",
    "from core.utils import print_config\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------#\n",
    "# Define model settings here.\n",
    "CONFIG_PATH = '../example/conf/config_dhbv_1_0.yaml'\n",
    "#------------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "# 1. Load configuration dictionary of model parameters and options.\n",
    "config = load_config(CONFIG_PATH)\n",
    "config['mode'] = 'predict'  # <-- Confirm that we are doing forwward if not set in the config file.\n",
    "print_config(config)\n",
    "\n",
    "# 2. Initialize physical model and NN.\n",
    "device = config['device']\n",
    "phy_model = hbv(config['dpl_model']['phy_model'])\n",
    "nn = init_nn_model(phy_model, config['dpl_model'])\n",
    "\n",
    "# 3. Load and initialize a dataset dictionary of NN and HBV model inputs.\n",
    "# Take a sample to reduce size on GPU.\n",
    "dataset_dict = HydroDataLoader(config).dataset\n",
    "dataset_sample = take_data_sample(config, dataset_dict, days=730, basins=100)\n",
    "\n",
    "# 4. Create the differentiable model dHBV 1.0: a torch.nn.Module describing how \n",
    "# the NN is linked to the physical model HBV.\n",
    "dpl_model = dHBV(phy_model=phy_model, nn_model=nn).to(device)\n",
    "\n",
    "## From here, forward or train dpl_model just as any torch.nn.Module model.\n",
    "\n",
    "# 5. For example, to forward:\n",
    "output = dpl_model.forward(dataset_sample)\n",
    "\n",
    "\n",
    "print(\"-------------\\n\")\n",
    "print(f\"Streamflow predictions for {output['flow_sim'].shape[0]} days and \" \\\n",
    "      f\"{output['flow_sim'].shape[1]} basins:\\nShowing the first 5 days for \" \\\n",
    "        f\"5 basins \\n {output['flow_sim'][:3,:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Training $\\delta$ HBV 1.0 -- Walkthrough\n",
    "\n",
    "Now that we can build $\\delta$ HBV 1.0, we proceed to train the model and expose critical steps in the process below.\n",
    "\n",
    "See [here](#3-training--hbv-10----abbreviated) to skip the walkthrough.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 2.1 Load the Config and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../deltaModel')  # Add the root directory of deltaModel\n",
    "\n",
    "from example import load_config \n",
    "from core.data.data_loaders.loader_hydro import HydroDataLoader\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------#\n",
    "# Define model settings here.\n",
    "CONFIG_PATH = '../example/conf/config_dhbv_1_0.yaml'\n",
    "#------------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "# Load configuration dictionary of model parameters and options.\n",
    "config = load_config(CONFIG_PATH)\n",
    "config['mode'] = 'train'  # <-- Confirm that we are doing forwward if not set in the config file.\n",
    "\n",
    "# Get training dataset\n",
    "train_dataset = HydroDataLoader(config, test_split=True).train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Initialize a $\\delta$ HBV 1.0, Optimizer, and Loss Function\n",
    "\n",
    "These are the auxillary tasks completed by the Trainer before beginning the training loop.\n",
    "\n",
    "\n",
    "<!-- We use the Adadelta optimizer from PyTorch, feeding it both learnable model\n",
    "parameters and a learning rate from the config file.\n",
    "\n",
    "\n",
    "Dynamically load the loss function identified in the config (RMSE for\n",
    "dHBV 1.0 and NSE for dHBV 1.1p). -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is our dHBV framework: \n",
      " ----- \n",
      " DeltaModel(\n",
      "  (phy_model): HBV()\n",
      "  (nn_model): CudnnLstmModel(\n",
      "    (linearIn): Linear(in_features=38, out_features=256, bias=True)\n",
      "    (lstm): CudnnLstm()\n",
      "    (linearOut): Linear(in_features=256, out_features=210, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from hydroDL2.models.hbv.hbv import HBV as hbv\n",
    "from models.loss_functions import get_loss_func\n",
    "from models.neural_networks import init_nn_model\n",
    "from models.differentiable_model import DeltaModel as dHBV\n",
    "\n",
    "\n",
    "# Initialize physical model and neural network\n",
    "phy_model = hbv(config['dpl_model']['phy_model'])\n",
    "nn = init_nn_model(phy_model, config['dpl_model'])\n",
    "\n",
    "# Create the differentiable model dHBV: \n",
    "device = config['device']\n",
    "dpl_model = dHBV(phy_model=phy_model, nn_model=nn).to(device)\n",
    "print(f\"Here is our dHBV framework: \\n ----- \\n {dpl_model}\")\n",
    "\n",
    "# Init an Adadelta optimizer\n",
    "optimizer = torch.optim.Adadelta(\n",
    "    dpl_model.parameters(),\n",
    "    lr=config['dpl_model']['nn_model']['learning_rate'],\n",
    ")\n",
    "\n",
    "# init a loss function\n",
    "loss_func = get_loss_func(train_dataset['target'], config['loss_function'], device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Train the Model\n",
    "\n",
    "Below we use a basic training loop to train the LSTM in $\\delta$ HBV 1.0 to optimize HBV's parameters and streamflow predictions.\n",
    "\n",
    "#### Key Steps in the Training Loop\n",
    "1. **Calculate Training Parameters**  \n",
    "   The `calc_training_params` function calculates the training settings:\n",
    "   - `n_sites`: The number of unique locations/sites in the dataset.\n",
    "   - `n_minibatch`: The number of samples to process per epoch.\n",
    "   - `n_timesteps`: The number of timesteps per sample.\n",
    "\n",
    "2. **Epoch Loop**  \n",
    "   Each epoch represents one full cycle through the training data. For each epoch:\n",
    "   - `total_loss` is reset to track the total error across all batches within the epoch.\n",
    "\n",
    "3. **Batch Loop**  \n",
    "   Within each epoch, the code processes data in smaller chunks (minibatches) to improve training efficiency and avoid\n",
    "   oversaturation of GPU VRAM. \n",
    "   \n",
    "   For each batch:\n",
    "   - **Sample Data**: `HydroDataSampler` randomly selects a sample of training data for the batch.\n",
    "   - **Forward Pass**: The model processes the input data to produce predictions.\n",
    "   - **Calculate Loss**: `loss_func` compares predictions to observed values to calculate the error for the batch.\n",
    "   - **Backward Pass and Optimization**: \n",
    "     - `loss.backward()` computes gradients to adjust the model’s parameters.\n",
    "     - `optimizer.step()` updates the LSTM parameters.\n",
    "     - `optimizer.zero_grad()` resets gradients for the next batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from core.data import create_training_grid\n",
    "from core.data.data_samplers.sampler_hydro import HydroDataSampler\n",
    "from core.utils import save_model\n",
    "\n",
    "\n",
    "# Initialize training sampler.\n",
    "sampler = HydroDataSampler(config)\n",
    "\n",
    "# Get target variable for training.\n",
    "target = config['train']['target'][0]\n",
    "\n",
    "# Number of training samples per epoch, batch size, and number of timesteps.\n",
    "n_samples, n_minibatch, n_timesteps = create_training_grid(\n",
    "    train_dataset['xc_nn_norm'],\n",
    "    config,\n",
    ")\n",
    "\n",
    "# Start of training.\n",
    "for epoch in range(1, config['train']['epochs'] + 1):\n",
    "    total_loss = 0.0  # Initialize epoch loss to zero.\n",
    "\n",
    "    prog_str = f\"Epoch {epoch}/{config['train']['epochs']}\"\n",
    "\n",
    "    # Work through training data in batches.\n",
    "    for i in tqdm.tqdm(range(1, n_minibatch + 1), desc=prog_str,\n",
    "                       leave=False, dynamic_ncols=True):\n",
    "        \n",
    "        # Take a sample of the training data for the batch.\n",
    "        dataset_sample = sampler.get_training_sample(\n",
    "            train_dataset,\n",
    "            n_samples,\n",
    "            n_timesteps,\n",
    "        )\n",
    "\n",
    "        # Forward pass through dPL model.\n",
    "        predictions = dpl_model.forward(dataset_sample)        \n",
    "        \n",
    "        # Calculate loss.\n",
    "        loss = loss_func(\n",
    "            predictions[target],\n",
    "            dataset_sample['target'],\n",
    "            n_samples=dataset_sample['batch_sample'],\n",
    "        )\n",
    "                                   \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / n_minibatch + 1\n",
    "    print(f\"Avg model loss after epoch {epoch}: {avg_loss}\")\n",
    "\n",
    "    # Save the model every save_epoch (set in the config).\n",
    "    model_name = config['dpl_model']['phy_model']['model']\n",
    "    if epoch % config['train']['save_epoch'] == 0:\n",
    "        save_model(config, dpl_model, model_name, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Training $\\delta$ HBV 1.0 -- Abbreviated\n",
    "\n",
    "Now we demonstrate the high level training loop for $\\delta$ HBV 1.0 in the code block below.\n",
    "\n",
    "--> For default settings with 50 training epochs, expect train times of ~8 hours with an Nvidia RTX 3090.\n",
    "\n",
    "**Note**\n",
    "- The settings defined in the config `../example/conf/config_dhbv_1_0.yaml` are set to replecate benchmark performance.\n",
    "- For model training, set `mode: train` in the config, or modify after config dict has been created (see below).\n",
    "- An `example/results/` directory will be generated to store experiment and model files. This location can be adjusted by changing the `save_path` key in your config. \n",
    "- The default training window from 1 October 1999 to 30 September 2008 with `batch_size=100` should use ~2.8GB of vram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mCurrent Configuration\u001b[0m\n",
      "  Experiment Mode:    train               \n",
      "  Model 1:            HBV                 \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data Source:        camels_531          \n",
      "  Train Range :       1999/10/01          2008/09/30          \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Train Epochs:       100                 Batch Size:         100                 \n",
      "  Dropout:            0.5                 Hidden Size:        256                 \n",
      "  Warmup:             365                 Concurrent Models:  16                  \n",
      "  Loss Fn:            RmseLossComb        \n",
      "\n",
      "\u001b[1mMachine\u001b[0m\n",
      "  Use Device:         cuda:0              \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 41\u001b[0m\n\u001b[1;32m     33\u001b[0m trainer \u001b[38;5;241m=\u001b[39m trainer(\n\u001b[1;32m     34\u001b[0m     config,\n\u001b[1;32m     35\u001b[0m     model,\n\u001b[1;32m     36\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m data_loader\u001b[38;5;241m.\u001b[39mtrain_dataset,\n\u001b[1;32m     37\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# 5. Start model training.\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/mhpi/lglonz/project_silmaril/generic_deltaModel/deltaModel/trainers/trainer.py:220\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    217\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(dataset_sample)\n\u001b[1;32m    218\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcalc_loss(dataset_sample)\n\u001b[0;32m--> 220\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/projects/mhpi/lglonz/project_silmaril/generic_deltaModel/venv/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/mhpi/lglonz/project_silmaril/generic_deltaModel/venv/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/mhpi/lglonz/project_silmaril/generic_deltaModel/venv/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../deltaModel')  # Add the dMG root directory.\n",
    "\n",
    "from example import load_config \n",
    "from models.model_handler import ModelHandler as dHBV\n",
    "from core.utils import print_config\n",
    "from core.utils.module_loaders import get_data_loader, get_trainer\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------#\n",
    "# Define model settings here.\n",
    "CONFIG_PATH = '../example/conf/config_dhbv_1_0.yaml'\n",
    "#------------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "# 1. Load configuration dictionary of model parameters and options.\n",
    "config = load_config(CONFIG_PATH)\n",
    "config['mode'] = 'train'  # <-- Confirm that we are doing training if not set in the config file.\n",
    "print_config(config)\n",
    "\n",
    "# 2. Initialize the differentiable HBV 1.1p model (LSTM + HBV 1.1p).\n",
    "model = dHBV(config, verbose=True)\n",
    "\n",
    "# 3. Load and initialize a dataset dictionary of NN and HBV model inputs.\n",
    "data_loader = get_data_loader(config['data_loader'])\n",
    "data_loader = data_loader(config, test_split=True, overwrite=False)\n",
    "\n",
    "# 4. Initialize trainer to handle model training.\n",
    "trainer = get_trainer(config['trainer'])\n",
    "trainer = trainer(\n",
    "    config,\n",
    "    model,\n",
    "    train_dataset = data_loader.train_dataset,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# 5. Start model training.\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Evaluate Model Performance\n",
    "\n",
    "After completing the training in [3.1](#31-training--hbv-10----abbreviated), or with the trained model provided, test $\\delta$ HBV 1.0 below on the evaluation data.\n",
    "\n",
    "--> For default settings expect evaluation time of ~5 minutes with an Nvidia RTX 3090.\n",
    "\n",
    "**Note**\n",
    "- For model evaluation, set `mode: test` in `example/conf/config_dhbv_1_0.yaml`, or modify after the config dict has been created (see below).\n",
    "- When evaluating provided models, confirm that `test: test_epoch` in the config corresponds to your desired model (50 or 100 epochs).\n",
    "- The default evaluation window from 1 October 1989 to 30 September 1999 with `batch_size=25` should use ~2.7GB of vram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mCurrent Configuration\u001b[0m\n",
      "  Experiment Mode:    test                \n",
      "  Model 1:            HBV                 \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data Source:        camels_531          \n",
      "  Test Range :        1989/10/01          1999/09/30          \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Train Epochs:       100                 Batch Size:         100                 \n",
      "  Dropout:            0.5                 Hidden Size:        256                 \n",
      "  Warmup:             365                 Concurrent Models:  16                  \n",
      "  Loss Fn:            RmseLossComb        \n",
      "\n",
      "\u001b[1mMachine\u001b[0m\n",
      "  Use Device:         cuda:0              \n",
      "\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/22 [00:00<?, ?it/s]/projects/mhpi/lglonz/project_silmaril/generic_deltaModel/example/hydrology/../../deltaModel/models/neural_networks/lstm_models.py:104: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1410.)\n",
      "  output, hy, cy, reserve, new_weight_buf = torch._cudnn_rnn(\n",
      "                                                       \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# 5. Start testing the model.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvaluating model...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/mhpi/lglonz/project_silmaril/generic_deltaModel/deltaModel/trainers/trainer.py:271\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_batch \u001b[38;5;241m=\u001b[39m i\n\u001b[1;32m    265\u001b[0m dataset_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampler\u001b[38;5;241m.\u001b[39mget_validation_sample(\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_dataset,\n\u001b[1;32m    267\u001b[0m     batch_start[i],\n\u001b[1;32m    268\u001b[0m     batch_end[i],\n\u001b[1;32m    269\u001b[0m )\n\u001b[0;32m--> 271\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43meval\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# Save the batch predictions\u001b[39;00m\n\u001b[1;32m    274\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdpl_model\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphy_model\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/projects/mhpi/lglonz/project_silmaril/generic_deltaModel/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/mhpi/lglonz/project_silmaril/generic_deltaModel/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/projects/mhpi/lglonz/project_silmaril/generic_deltaModel/example/hydrology/../../deltaModel/models/model_handler.py:196\u001b[0m, in \u001b[0;36mModelHandler.forward\u001b[0;34m(self, dataset_dict, eval)\u001b[0m\n\u001b[1;32m    194\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 196\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dict[name] \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;66;03m## Training mode\u001b[39;00m\n\u001b[1;32m    199\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m/projects/mhpi/lglonz/project_silmaril/generic_deltaModel/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/mhpi/lglonz/project_silmaril/generic_deltaModel/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/projects/mhpi/lglonz/project_silmaril/generic_deltaModel/example/hydrology/../../deltaModel/models/differentiable_model.py:132\u001b[0m, in \u001b[0;36mDeltaModel.forward\u001b[0;34m(self, data_dict)\u001b[0m\n\u001b[1;32m    129\u001b[0m     parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_model(data_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxc_nn_norm\u001b[39m\u001b[38;5;124m'\u001b[39m])        \n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# Physics model\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphy_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "File \u001b[0;32m/projects/mhpi/lglonz/project_silmaril/generic_deltaModel/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/mhpi/lglonz/project_silmaril/generic_deltaModel/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/projects/mhpi/lglonz/project_silmaril/hydroDL2/src/hydroDL2/models/hbv/hbv.py:291\u001b[0m, in \u001b[0;36mHBV.forward\u001b[0;34m(self, x_dict, parameters)\u001b[0m\n\u001b[1;32m    285\u001b[0m phy_params_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdescale_phy_parameters(\n\u001b[1;32m    286\u001b[0m     phy_params[warm_up:,:,:],\n\u001b[1;32m    287\u001b[0m     dy_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic_params\n\u001b[1;32m    288\u001b[0m )\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m# Run the model for the remainder of simulation period.\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPBM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mwarm_up\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[43mSNOWPACK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMELTWATER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSUZ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSLZ\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m            \u001b[49m\u001b[43mphy_params_dict\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/mhpi/lglonz/project_silmaril/hydroDL2/src/hydroDL2/models/hbv/hbv.py:368\u001b[0m, in \u001b[0;36mHBV.PBM\u001b[0;34m(self, forcing, states, full_param_dict)\u001b[0m\n\u001b[1;32m    366\u001b[0m melt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(melt, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m    367\u001b[0m \u001b[38;5;66;03m# melt[melt > SNOWPACK] = SNOWPACK[melt > SNOWPACK]\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m melt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmelt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSNOWPACK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m MELTWATER \u001b[38;5;241m=\u001b[39m MELTWATER \u001b[38;5;241m+\u001b[39m melt\n\u001b[1;32m    370\u001b[0m SNOWPACK \u001b[38;5;241m=\u001b[39m SNOWPACK \u001b[38;5;241m-\u001b[39m melt\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../deltaModel')  # Add the dMG root directory.\n",
    "\n",
    "from example import load_config \n",
    "from models.model_handler import ModelHandler as dHBV\n",
    "from core.utils import print_config\n",
    "from core.utils.module_loaders import get_data_loader, get_trainer\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------#\n",
    "# Define model settings here.\n",
    "CONFIG_PATH = '../example/conf/config_dhbv_1_0.yaml'\n",
    "#------------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "# 1. Load configuration dictionary of model parameters and options.\n",
    "config = load_config(CONFIG_PATH)\n",
    "config['mode'] = 'test'  # <-- Confirm that we are doing testing if not set in the config file.\n",
    "print_config(config)\n",
    "\n",
    "# 2. Initialize the differentiable HBV 1.1p model (LSTM + HBV 1.1p).\n",
    "model = dHBV(config, verbose=True)\n",
    "\n",
    "# 3. Load and initialize a dataset dictionary of NN and HBV model inputs.\n",
    "data_loader = get_data_loader(config['data_loader'])\n",
    "data_loader = data_loader(config, test_split=True, overwrite=False)\n",
    "\n",
    "# 4. Initialize trainer to handle model evaluation.\n",
    "trainer = get_trainer(config['trainer'])\n",
    "trainer = trainer(\n",
    "    config,\n",
    "    model,\n",
    "    eval_dataset = data_loader.eval_dataset,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# 5. Start testing the model.\n",
    "print('Evaluating model...')\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Trained Model Performance\n",
    "\n",
    "After running evaluation on the model, a new directory (e.g., for a model trained for 50 epochs and tested from years 1989-1999), `test1989-1999_Ep50/` will be created in the same directory containing the model files. This path will be populated with...\n",
    "\n",
    "1. All model outputs (fluxes, states), including the target variable, *streamflow* (`flow_sim.npy`),\n",
    "\n",
    "2. `flow_sim_obs`, streamflow observation data for comparison against model predictions,\n",
    "\n",
    "2. `metrics.json`, containing evaluation metrics accross the test time range for every gage in the dataset,\n",
    "\n",
    "3. `metrics_agg.json`, containing evaluation metrics statistics across all gages (mean, median, standar deviation).\n",
    "\n",
    "4. `normalization_statistics.json`, containing statistics used for normalizing the testing data.\n",
    "\n",
    "\n",
    "We can use these outputs to visualize $\\delta$ HBV 1.0's performance with a \n",
    "1. Cumulative distribution function (CDF) plot, \n",
    "\n",
    "2. CONUS map of gage locations and metric (e.g., NSE) performance.\n",
    "\n",
    "<br>\n",
    "\n",
    "But first, let's first check the (basin-)aggregated metrics for NSE, KGE, bias, RMSE, and, for both high/low flow regimes, RMSE and absolute percent bias..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from core.post import print_metrics\n",
    "from core.data import load_json\n",
    "\n",
    "\n",
    "print(f\"Evaluation output files saved to: {config['out_path']} \\n\")\n",
    "\n",
    "\n",
    "# 1. Load the basin-aggregated evaluation results.\n",
    "metrics_path = os.path.join(config['out_path'], 'metrics_agg.json')\n",
    "metrics = load_json(metrics_path)\n",
    "print(f\"Available metrics: {metrics.keys()} \\n\")\n",
    "\n",
    "# 2. Print the evaluation results.\n",
    "metric_names =  [\n",
    "    # Choose metrics to show.\n",
    "    'nse', 'kge', 'bias', 'rmse', 'rmse_low', 'rmse_high', 'flv_abs', 'fhv_abs',\n",
    "]\n",
    "print_metrics(metrics, metric_names, mode='median', precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the CDF Plot\n",
    "\n",
    "The cumulative distribution function (CDF) plot tells us what percentage (CDF on the y-axis) of basins performed at least better than a given metric on the evaluation data.\n",
    "We give an example of such a plot below for NSE, but you can adjust this to your preferred metric. See the output from the previous cell to see what metrics are available. (Note some may require changing `xbounds` in the `plot_cdf`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot CDF of the evaluation results.\n",
    "from core.post.plot_cdf import plot_cdf\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------#\n",
    "# Choose the metric to plot. (See available metrics printed above, or in the metrics_agg.json file).\n",
    "METRIC = 'nse'\n",
    "#------------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "# 1. Load the evaluation metrics.\n",
    "metrics_path = os.path.join(config['out_path'], 'metrics.json')\n",
    "metrics = load_json(metrics_path)\n",
    "\n",
    "# 2. Plot the CDF for NSE.\n",
    "plot_cdf(\n",
    "    metrics=[metrics],\n",
    "    metric_names=[METRIC],\n",
    "    model_labels=['dHBV 1.1p'],\n",
    "    title=\"CDF of NSE for $\\delta$HBV 1.1p\",\n",
    "    xlabel=METRIC.capitalize(),\n",
    "    figsize=(8, 6),\n",
    "    xbounds=(0, 1),\n",
    "    ybounds=(0, 1),\n",
    "    show_arrow=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the Spatial Plot\n",
    "\n",
    "This plot shows the locations of each basin in the evaluation data, color-coded by performance on a metric. Here we give a plot for NSE, but as before, this metric can be changed to your preference. (See above for what is available; for metrics not valued between 0 and 1, you will need to set `dynamic_colorbar=True` in `geoplot_single_metric` to ensure proper coding.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the evaluation results spatially.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "from core.data import txt_to_array\n",
    "from core.post.plot_geo import geoplot_single_metric\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------#\n",
    "# Choose the metric to plot. (See available metrics printed above, or in the metrics_agg.json file).\n",
    "METRIC = 'nse'\n",
    "\n",
    "# Set the paths to the gage id lists and shapefiles...\n",
    "GAGE_ID_PATH = 'your/path/to/gageid.npy'\n",
    "GAGE_ID_531_PATH = 'your/path/to/Sub531ID.txt'\n",
    "SHAPEFILE_PATH = 'your/path/to/camels_671_loc.shp'\n",
    "#------------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "# 2. Load gage ids + basin shapefile with geocoordinates (lat, long) for every gage.\n",
    "gage_ids = np.load(GAGE_ID_PATH, allow_pickle=True)\n",
    "gage_ids_531 = txt_to_array(GAGE_ID_531_PATH)\n",
    "coords = gpd.read_file(SHAPEFILE_PATH)\n",
    "\n",
    "# 3. Format geocoords for 531- and 671-basin CAMELS sets.\n",
    "coords_531 = coords[coords['gage_id'].isin(list(gage_ids_531))].copy()\n",
    "\n",
    "coords['gage_id'] = pd.Categorical(coords['gage_id'], categories=list(gage_ids), ordered=True)\n",
    "coords_531['gage_id'] = pd.Categorical(coords_531['gage_id'], categories=list(gage_ids_531), ordered=True)\n",
    "\n",
    "coords = coords.sort_values('gage_id')  # Sort to match order of metrics.\n",
    "basin_coords_531 = coords_531.sort_values('gage_id')\n",
    "\n",
    "# 4. Load the evaluation metrics.\n",
    "metrics_path = os.path.join(config['out_path'], 'metrics.json')\n",
    "metrics = load_json(metrics_path)\n",
    "\n",
    "# 5. Add the evaluation metrics to the basin shapefile.\n",
    "if config['observations']['name'] == 'camels_671':\n",
    "    coords[METRIC] = metrics[METRIC]\n",
    "    full_data = coords\n",
    "elif config['observations']['name'] == 'camels_531':\n",
    "    coords_531[METRIC] = metrics[METRIC]\n",
    "    full_data = coords_531\n",
    "else:\n",
    "    raise ValueError(f\"Observation data supported: 'camels_671' or 'camels_531'. Got: {config['observations']}\")\n",
    "\n",
    "# 6. Plot the evaluation results spatially.\n",
    "geoplot_single_metric(\n",
    "    full_data,\n",
    "    METRIC,\n",
    "    f\"Spatial Map of {METRIC.upper()} for $\\delta$HBV 1.1p on CAMELS \" \\\n",
    "        f\"{config['observations']['name'].split('_')[-1]}\",\n",
    "    dynamic_colorbar=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Forward $\\delta$ HBV 1.0\n",
    "\n",
    "After completing [these](#before-running) steps, forward the $\\delta$ HBV 1.0 model with the code block below. This is intended to be an abbreviation of the forward demonstration in [1.3](#13-demonstration).\n",
    "\n",
    "Note:\n",
    "- The settings defined in `../example/conf/config_dhbv_1_0.yaml` are set to replecate benchmark performance.\n",
    "- The default inference window is set from 1 October 2012 to 30 September 2014, which should use ~2.7GB of vram.\n",
    "- The first year (`warm_up` in the config, 365 days is default) of the inference period is used for initializing HBV's internal states (water storages) and is, therefore, excluded from the model's prediction output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../deltaModel')  # Add the dMG root directory.\n",
    "\n",
    "from example import load_config \n",
    "from models.model_handler import ModelHandler as dHBV\n",
    "from core.utils import print_config\n",
    "from core.utils.module_loaders import get_data_loader, get_trainer\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------#\n",
    "# Define model settings here.\n",
    "CONFIG_PATH = '../example/conf/config_dhbv_1_1p.yaml'\n",
    "#------------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "# 1. Load configuration dictionary of model parameters and options.\n",
    "config = load_config(CONFIG_PATH)\n",
    "config['mode'] = 'predict'  # <-- Confirm that we are doing training if not set in the config file.\n",
    "print_config(config)\n",
    "\n",
    "# 2. Initialize the differentiable HBV 1.1p model (LSTM + HBV 1.1p).\n",
    "model = dHBV(config, verbose=True)\n",
    "\n",
    "# 3. Load and initialize a dataset dictionary of NN and HBV model inputs.\n",
    "data_loader = get_data_loader(config['data_loader'])\n",
    "data_loader = data_loader(config, test_split=False, overwrite=False)\n",
    "dataset = data_loader.dataset\n",
    "\n",
    "# 4. Forward the model to get the predictions.\n",
    "predictions = model.forward(dataset, eval=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Model Predictions\n",
    "\n",
    "After running model inference we can, e.g., view the hydrograph for one of the basins to see we are getting expected outputs.\n",
    "\n",
    "We can do this with our target variable, streamflow, for instance... (though, there are many other states and fluxes we can output as shown in the output cell below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from core.utils.dates import Dates\n",
    "from core.data import txt_to_array\n",
    "from core.post.plot_hydrograph import plot_hydrograph\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------#\n",
    "# Choose a basin by USGS gage ID to plot.\n",
    "GAGE_ID = 1022500\n",
    "TARGET = 'flow_sim'\n",
    "\n",
    "# Resample to 3-day prediction. Options: 'D', 'W', 'M', 'Y'.\n",
    "RESAMPLE = '3D'\n",
    "\n",
    "# Set the paths to the gage ID lists...\n",
    "GAGE_ID_PATH = 'your/path/to/gageid.npy'\n",
    "GAGE_ID_531_PATH = 'your/path/to/Sub531ID.txt'\n",
    "#------------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "print(f\"HBV states and fluxes: {predictions['HBV_1_1p'].keys()} \\n\")\n",
    "\n",
    "\n",
    "# 1. Get the streamflow predictions and daily timesteps of the prediction window.\n",
    "pred = predictions['HBV_1_1p'][TARGET]\n",
    "timesteps = Dates(config['predict'], config['dpl_model']['rho']).batch_daily_time_range\n",
    "\n",
    "# Remove warm-up period to match model output (see Note above.)\n",
    "timesteps = timesteps[config['dpl_model']['phy_model']['warm_up']:]\n",
    "\n",
    "\n",
    "# 2. Load the gage ID lists and get the basin index.\n",
    "gage_ids = np.load(GAGE_ID_PATH, allow_pickle=True)\n",
    "gage_ids_531 = txt_to_array(GAGE_ID_531_PATH)\n",
    "\n",
    "print(f\"First 20 available gage IDs: \\n {gage_ids[:20]} \\n\")\n",
    "print(f\"First 20 available gage IDs (531 subset): \\n {gage_ids_531[:20]} \\n\")\n",
    "\n",
    "if config['observations']['name'] == 'camels_671':\n",
    "    if GAGE_ID in gage_ids:\n",
    "        basin_idx = list(gage_ids).index(GAGE_ID)\n",
    "    else:\n",
    "        raise ValueError(f\"Basin with gage ID {GAGE_ID} not found in the CAMELS 671 dataset.\")\n",
    "\n",
    "elif config['observations']['name'] == 'camels_531':\n",
    "    if GAGE_ID in gage_ids_531:\n",
    "        basin_idx = list(gage_ids_531).index(GAGE_ID)\n",
    "    else:\n",
    "        raise ValueError(f\"Basin with gage ID {GAGE_ID} not found in the CAMELS 531 dataset.\")\n",
    "else:\n",
    "    raise ValueError(f\"Observation data supported: 'camels_671' or 'camels_531'. Got: {config['observations']}\")\n",
    "\n",
    "\n",
    "# 3. Get the data for the chosen basin and plot.\n",
    "streamflow_pred_basin = pred[:, basin_idx].squeeze()\n",
    "\n",
    "plot_hydrograph(\n",
    "    timesteps,\n",
    "    streamflow_pred_basin,\n",
    "    streamflow_pred_basin,\n",
    "    resample=RESAMPLE,\n",
    "    title=f\"Hydrograph for Gage ID {GAGE_ID}\",\n",
    "    ylabel='Streamflow (ft$^3$/s)',\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
