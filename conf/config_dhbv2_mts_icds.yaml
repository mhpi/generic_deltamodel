# This configuration file is specifically for the high-resolution, multiscale, differentiable hydrologic model, dHBV2.0UH, from Song et al. (2025). If you find this code is useful to your work, please cite the paper below:
# Yalan Song, Tadd Bindas, Chaopeng Shen, et al. "High-resolution national-scale water modeling is enhanced by multiscale differentiable physics-informed machine learning." Water Resources Research (2025). https://doi.org/10.1029/2024WR038928.

defaults:
   - _self_
   - hydra: settings
   - observations: hydrofabric


## General -------------------------------#
mode: train  # 'train', 'train_test', 'test', or 'simulation'
random_seed: 42
device: cuda
gpu_id: 0

data_loader: MtsHydroLoader
data_sampler: MtsHydroSampler
trainer: MtsTrainer

save_path: /storage/group/cxs1024/default/wby5078/data/distributed-ds-camels/trained_models/h-dhbv2_6/
    # NOTE: The output path is relative to the current working directory.
    #       If you want to save the output in a different directory, please specify the absolute path.
    #       * If a valid trained_model path is specified, save_path will be set to trained_model.
model_path: /storage/group/cxs1024/default/wby5078/data/distributed-ds-camels/trained_models/h-dhbv2_6/model/  # Pretrained dHBV 2.0 (+ input data) can be downloaded from: https://mhpi-spatial.s3.us-east-2.amazonaws.com/mhpi-release/models/dHBV_2_0_trained.zip


## Training ------------------------------#
train:
    # NOTE: Multiscale training for dHBV2.0 is not currently enabled in dMG.
    # Training code will be released at a later time.
    start_time: 1991/01/01
    end_time: 2003/12/31
    target: [streamflow]
    optimizer: Adam  # Adadelta, Adam
    batch_size: 100
    epochs: 30
    start_epoch: 0
    save_epoch: 1
    stride: 168
    chunk_year_size : 1


## Validation -------------------------------#
valid:
    start_time: 2004/01/01
    end_time: 2008/12/31
    batch_size: 100


## Testing (also used fro seamless simulation) -------------------------------#
test:
    start_time: 2009/01/01
    end_time: 2018/12/31
    batch_size: 100
    test_epoch: 20


## Loss Function -------------------------#
loss_function:
    model: WeightedNSELossBatch
    loss_fn_params:
      alpha: 0.0
      gamma: 1.0
      max_w: 1.0


## Differentiable Model -----------------------------#
delta_model:

    phy_model:

        name: [Hbv_2_mts]

        ## Citations ##
        # HBV 2.0: Yalan Song, Tadd Bindas, Chaopeng Shen, et al.
        # "High-resolution national-scale water modeling is enhanced by
        # multiscale differentiable physics-informed machine learning." Water
        # Resources Research (2025). https://doi.org/10.1029/2024WR038928.

        high_freq_model:
            model: [Hbv_2h]
            nmul: 1  # 4
            warm_up: 0
            warm_up_states: True
            dy_drop: 0.0
            dynamic_params:
                Hbv_2h: [parBETA, parK0, parBETAET]

            routing: False
            use_log_norm: []
            nearzero: 0.00001

            forcings: [
                P,
                Temp,
                PET,
            ]
            attributes: []

            window_size: 336  # high-freq model window size (hours)
            train_warmup: 168  # high-freq model warmup steps for training loss, also for routing
            agg_timescale_dim: [0, 2]  # indexes of P and PET in forcings, sum aggregation from high to low freq
            train_spatial_chunk_size: 32768  # number of unit-basins per sub-batch for high-freq model training
            simulate_spatial_chunk_size: 10000  # number of unit-basins per sub-batch for runoff generation during simulation
            simulate_temporal_chunk_size: 168  # number of time steps per sub-batch for runoff routing during simulation


        low_freq_model:
            model: [Hbv_2]
            nmul: 1  # 4
            warm_up: 0
            warm_up_states: True
            dy_drop: 0.0
            dynamic_params:
              Hbv_2: [parBETA, parK0, parBETAET]

            routing: False
            use_log_norm: []
            nearzero: 0.00001

            forcings: [
              P,
              Temp,
              PET,
            ]
            attributes: []

            window_size: 351

    nn_model:
        sub_batch_size: 100  # avoid cuda OOM for distributed model

        high_freq_model:
            model: LstmMlp2Model

            lstm_dropout: 0.1
            lstm_hidden_size: 32  # 64

            mlp_dropout: 0.1
            mlp_hidden_size: 64  # 4096

            mlp2_dropout: 0.1
            mlp2_hidden_size: 64  # 4096

            forcings: [
              P,
              Temp,
              PET,
            ]
            attributes: [
              aridity, meanP, ETPOT_Hargr, NDVI, FW, meanslope, SoilGrids1km_sand, SoilGrids1km_clay, SoilGrids1km_silt, glaciers, HWSD_clay, HWSD_gravel, HWSD_sand, HWSD_silt, meanelevation, meanTa, permafrost, permeability, seasonality_P, seasonality_PET, snow_fraction, snowfall_fraction, T_clay, T_gravel, T_sand, T_silt, Porosity, catchsize
            ]
            attributes2: [aridity, NDVI, FW, meanslope, SoilGrids1km_sand, SoilGrids1km_clay, SoilGrids1km_silt, glaciers, HWSD_clay, HWSD_gravel, HWSD_sand, HWSD_silt, meanelevation, permafrost, permeability, snow_fraction, T_clay, T_gravel, T_sand, T_silt, Porosity, catchsize, lengthkm]

        low_freq_model:
            model: LstmMlpModel

            lstm_dropout: 0.1
            lstm_hidden_size: 32  # 64

            mlp_dropout: 0.1
            mlp_hidden_size: 64  # 4096

            forcings: [
              P,
              Temp,
              PET,
            ]
            attributes: [
              aridity, meanP, ETPOT_Hargr, NDVI, FW, meanslope, SoilGrids1km_sand, SoilGrids1km_clay, SoilGrids1km_silt, glaciers, HWSD_clay, HWSD_gravel, HWSD_sand, HWSD_silt, meanelevation, meanTa, permafrost, permeability, seasonality_P, seasonality_PET, snow_fraction, snowfall_fraction, T_clay, T_gravel, T_sand, T_silt, Porosity, catchsize
            ]

        learning_rate: 0.001  # 1.0, 0.001
        lr_scheduler: None
        lr_scheduler_params:
            step_size: 10
            gamma: 0.5
